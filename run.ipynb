{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import click\n",
    "import yaml\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" trainer.py \"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from model.dataset import Dataset\n",
    "from model.fp.melspec.melspectrogram import get_melspec_layer\n",
    "from model.fp.specaug_chain.specaug_chain import get_specaug_chain_layer\n",
    "from model.fp.nnfp import get_fingerprinter\n",
    "from model.fp.NTxent_loss_single_gpu import NTxentLoss\n",
    "from model.fp.online_triplet_loss import OnlineTripletLoss\n",
    "from model.fp.lamb_optimizer import LAMB\n",
    "from model.utils.experiment_helper import ExperimentHelper\n",
    "from model.utils.mini_search_subroutines import mini_search_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_specaug: spec-augmentation layer.\n",
    "    m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "    assert(m_specaug.bypass==False) # Detachable by setting m_specaug.bypass.\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    return m_pre, m_specaug, m_fp\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(X, m_pre, m_specaug, m_fp, loss_obj, helper):\n",
    "    \"\"\" Train step \"\"\"\n",
    "    # X: (Xa, Xp)\n",
    "    # Xa: anchors or originals, s.t. [xa_0, xa_1,...]\n",
    "    # Xp: augmented replicas, s.t. [xp_0, xp_1] with xp_n = rand_aug(xa_n).\n",
    "    n_anchors = len(X[0])\n",
    "    X = tf.concat(X, axis=0)\n",
    "    feat = m_specaug(m_pre(X))  # (nA+nP, F, T, 1)\n",
    "    m_fp.trainable = True\n",
    "    with tf.GradientTape() as t:\n",
    "        emb = m_fp(feat)  # (BSZ, Dim)\n",
    "        loss, sim_mtx, _ = loss_obj.compute_loss(\n",
    "            emb[:n_anchors, :], emb[n_anchors:, :]) # {emb_org, emb_rep}\n",
    "    g = t.gradient(loss, m_fp.trainable_variables)\n",
    "    helper.optimizer.apply_gradients(zip(g, m_fp.trainable_variables))\n",
    "    avg_loss = helper.update_tr_loss(loss) # To tensorboard.\n",
    "    return avg_loss, sim_mtx # avg_loss: average within the current epoch\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def val_step(X, m_pre, m_fp, loss_obj, helper):\n",
    "    \"\"\" Validation step \"\"\"\n",
    "    n_anchors = len(X[0])\n",
    "    X = tf.concat(X, axis=0)\n",
    "    feat = m_pre(X)  # (nA+nP, F, T, 1)\n",
    "    m_fp.trainable = False\n",
    "    emb = m_fp(feat)  # (BSZ, Dim)\n",
    "    loss, sim_mtx, _ = loss_obj.compute_loss(\n",
    "        emb[:n_anchors, :], emb[n_anchors:, :]) # {emb_org, emb_rep}\n",
    "    avg_loss = helper.update_val_loss(loss) # To tensorboard.\n",
    "    return avg_loss, sim_mtx\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(X, m_pre, m_fp):\n",
    "    \"\"\" Test step used for mini-search-validation \"\"\"\n",
    "    X = tf.concat(X, axis=0)\n",
    "    feat = m_pre(X)  # (nA+nP, F, T, 1)\n",
    "    m_fp.trainable = False\n",
    "    emb_f = m_fp.front_conv(feat)  # (BSZ, Dim)\n",
    "    emb_f_postL2 = tf.math.l2_normalize(emb_f, axis=1)\n",
    "    emb_gf = m_fp.div_enc(emb_f)\n",
    "    emb_gf = tf.math.l2_normalize(emb_gf, axis=1)\n",
    "    return emb_f, emb_f_postL2, emb_gf # f(.), L2(f(.)), L2(g(f(.))\n",
    "\n",
    "\n",
    "def mini_search_validation(ds, m_pre, m_fp, mode='argmin',\n",
    "                           scopes=[1, 3, 5, 9, 11, 19], max_n_samples=3000):\n",
    "    \"\"\" Mini-search-validation \"\"\"\n",
    "    # Construct mini-DB\n",
    "    key_strs = ['f', 'L2(f)', 'g(f)']\n",
    "    m_fp.trainable = False\n",
    "    (db, query, emb, dim) = (dict(), dict(), dict(), dict())\n",
    "    dim['f'] = dim['L2(f)'] = m_fp.front_hidden_ch[-1]\n",
    "    dim['g(f)'] = m_fp.emb_sz\n",
    "    bsz = ds.bsz\n",
    "    n_anchor = bsz // 2\n",
    "    n_iter = min(len(ds), max_n_samples // bsz)\n",
    "    for k in key_strs:\n",
    "        (db[k], query[k]) = (tf.zeros((0, dim[k])), tf.zeros((0, dim[k])))\n",
    "    for i in range(n_iter):\n",
    "        X = ds.__getitem__(i)\n",
    "        emb['f'], emb['L2(f)'], emb['g(f)'] = test_step(X, m_pre, m_fp)\n",
    "        for k in key_strs:\n",
    "            db[k] = tf.concat((db[k], emb[k][:n_anchor, :]), axis=0)\n",
    "            query[k] = tf.concat((query[k], emb[k][n_anchor:, :]), axis=0)\n",
    "\n",
    "    # Search test\n",
    "    accs_by_scope = dict()\n",
    "    for k in key_strs:\n",
    "        tf.print(f'======= mini-search-validation: \\033[31m{mode} \\033[33m{k} \\033[0m=======' + '\\033[0m')\n",
    "        query[k] = tf.expand_dims(query[k], axis=1) # (nQ, d) --> (nQ, 1, d)\n",
    "        accs_by_scope[k], _ = mini_search_eval(\n",
    "            query[k], db[k], scopes, mode, display=True)\n",
    "    return accs_by_scope, scopes, key_strs\n",
    "\n",
    "\n",
    "def trainer(cfg, checkpoint_name):\n",
    "    # Dataloader\n",
    "    dataset = Dataset(cfg)\n",
    "\n",
    "    # Build models.\n",
    "    m_pre, m_specaug, m_fp = build_fp(cfg)\n",
    "\n",
    "    # Learning schedule\n",
    "    total_nsteps = cfg['TRAIN']['MAX_EPOCH'] * len(dataset.get_train_ds())\n",
    "    if cfg['TRAIN']['LR_SCHEDULE'].upper() == 'COS':\n",
    "        lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "            initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "            decay_steps=total_nsteps,\n",
    "            alpha=1e-06)\n",
    "    elif cfg['TRAIN']['LR_SCHEDULE'].upper() == 'COS-RESTART':\n",
    "        lr_schedule = tf.keras.experimental.CosineDecayRestarts(\n",
    "            initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "            first_decay_steps=int(total_nsteps * 0.1),\n",
    "            num_periods=0.5,\n",
    "            alpha=2e-06)\n",
    "    else:\n",
    "        lr_schedule = float(cfg['TRAIN']['LR'])\n",
    "\n",
    "    # Optimizer\n",
    "    if cfg['TRAIN']['OPTIMIZER'].upper() == 'LAMB':\n",
    "        opt = LAMB(learning_rate=lr_schedule)\n",
    "    elif cfg['TRAIN']['OPTIMIZER'].upper() == 'ADAM':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        raise NotImplementedError(cfg['TRAIN']['OPTIMIZER'])\n",
    "\n",
    "    # Experiment helper: see utils.experiment_helper.py for details.\n",
    "    helper = ExperimentHelper(\n",
    "        checkpoint_name=checkpoint_name,\n",
    "        optimizer=opt,\n",
    "        model_to_checkpoint=m_fp,\n",
    "        cfg=cfg)\n",
    "\n",
    "    # Loss objects\n",
    "    if cfg['LOSS']['LOSS_MODE'].upper() == 'NTXENT': # Default\n",
    "        loss_obj_train = NTxentLoss(\n",
    "            n_org=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "            n_rep=cfg['BSZ']['TR_BATCH_SZ'] - cfg['BSZ']['TR_N_ANCHOR'],\n",
    "            tau=cfg['LOSS']['TAU'])\n",
    "        loss_obj_val = NTxentLoss(\n",
    "            n_org=cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "            n_rep=cfg['BSZ']['VAL_BATCH_SZ'] - cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "            tau=cfg['LOSS']['TAU'])\n",
    "    elif cfg['LOSS']['LOSS_MODE'].upper() == 'ONLINE-TRIPLET': # Now-playing\n",
    "        loss_obj_train = OnlineTripletLoss(\n",
    "            bsz=cfg['BSZ']['TR_BATCH_SZ'],\n",
    "            n_anchor=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "            mode = 'semi-hard',\n",
    "            margin=cfg['LOSS']['MARGIN'])\n",
    "        loss_obj_val = OnlineTripletLoss(\n",
    "            bsz=cfg['BSZ']['VAL_BATCH_SZ'],\n",
    "            n_anchor=cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "            mode = 'all', # use 'all' mode for validation\n",
    "            margin=0.)\n",
    "    else:\n",
    "        raise NotImplementedError(cfg['LOSS']['LOSS_MODE'])\n",
    "\n",
    "    # Training loop\n",
    "    ep_start = helper.epoch\n",
    "    ep_max = cfg['TRAIN']['MAX_EPOCH']\n",
    "    for ep in range(ep_start, ep_max + 1):\n",
    "        tf.print(f'EPOCH: {ep}/{ep_max}')\n",
    "\n",
    "        # Train\n",
    "        \"\"\" Parallelism to speed up preprocessing.............. \"\"\"\n",
    "        train_ds = dataset.get_train_ds(cfg['DATA_SEL']['REDUCE_ITEMS_P'])\n",
    "        progbar = Progbar(len(train_ds))\n",
    "        enq = tf.keras.utils.OrderedEnqueuer(\n",
    "            train_ds, use_multiprocessing=True, shuffle=train_ds.shuffle)\n",
    "        enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "                  max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "        i = 0\n",
    "        while i < len(enq.sequence):\n",
    "            X = next(enq.get()) # X: Tuple(Xa, Xp)\n",
    "            avg_loss, sim_mtx = train_step(X, m_pre, m_specaug, m_fp,\n",
    "                                            loss_obj_train, helper)\n",
    "            progbar.add(1, values=[(\"tr loss\", avg_loss)])\n",
    "            i += 1\n",
    "        enq.stop()\n",
    "        \"\"\" End of Parallelism................................. \"\"\"\n",
    "\n",
    "        if cfg['TRAIN']['SAVE_IMG'] and (sim_mtx is not None):\n",
    "            helper.write_image_tensorboard('tr_sim_mtx', sim_mtx.numpy())\n",
    "\n",
    "        # Validate\n",
    "        \"\"\" Parallelism to speed up preprocessing.............. \"\"\"\n",
    "        val_ds = dataset.get_val_ds(max_song=250) # max 500\n",
    "        enq = tf.keras.utils.OrderedEnqueuer(\n",
    "            val_ds, use_multiprocessing=True, shuffle=False)\n",
    "        enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "                  max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "        i = 0\n",
    "        while i < len(enq.sequence):\n",
    "            X = next(enq.get()) # X: Tuple(Xa, Xp)\n",
    "            _, sim_mtx = val_step(X, m_pre, m_fp, loss_obj_val,\n",
    "                                  helper)\n",
    "            i += 1\n",
    "        enq.stop()\n",
    "        \"\"\" End of Parallelism................................. \"\"\"\n",
    "\n",
    "        if cfg['TRAIN']['SAVE_IMG'] and (sim_mtx is not None):\n",
    "            helper.write_image_tensorboard('val_sim_mtx', sim_mtx.numpy())\n",
    "\n",
    "        # On epoch end\n",
    "        tf.print('tr_loss:{:.4f}, val_loss:{:.4f}'.format(\n",
    "            helper._tr_loss.result(), helper._val_loss.result()))\n",
    "        helper.update_on_epoch_end(save_checkpoint_now=True)\n",
    "\n",
    "\n",
    "        # Mini-search-validation (optional)\n",
    "        if cfg['TRAIN']['MINI_TEST_IN_TRAIN']:\n",
    "            accs_by_scope, scopes, key_strs = mini_search_validation(\n",
    "                val_ds, m_pre, m_fp)\n",
    "            for k in key_strs:\n",
    "                helper.update_minitest_acc(accs_by_scope[k], scopes, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_fname):\n",
    "    config_filepath = './config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def update_config(cfg, key1: str, key2: str, val):\n",
    "    cfg[key1][key2] = val\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def print_config(cfg):\n",
    "    os.system(\"\")\n",
    "    print('\\033[36m' + yaml.dump(cfg, indent=4, width=120, sort_keys=False) +\n",
    "          '\\033[0m')\n",
    "    return\n",
    "\n",
    "\n",
    "def train(checkpoint_name, config, max_epoch):\n",
    "    \"\"\" Train a neural audio fingerprinter.\n",
    "\n",
    "    ex) python run.py train CHECKPOINT_NAME --max_epoch=100\n",
    "\n",
    "        # with custom config file\n",
    "        python run.py train CHECKPOINT_NAME --max_epoch=100 -c CONFIG_NAME\n",
    "\n",
    "    NOTE: If './LOG_ROOT_DIR/checkpoint/CHECKPOINT_NAME already exists, the training will resume from the latest checkpoint in the directory.\n",
    "\n",
    "    \"\"\"\n",
    "    from model.utils.config_gpu_memory_lim import allow_gpu_memory_growth\n",
    "    from model.trainer import trainer\n",
    "\n",
    "    cfg = load_config(config)\n",
    "    if max_epoch: update_config(cfg, 'TRAIN', 'MAX_EPOCH', max_epoch)\n",
    "    print_config(cfg)\n",
    "    # allow_gpu_memory_growth()\n",
    "    trainer(cfg, checkpoint_name)\n",
    "\n",
    "\"\"\" Generate fingerprint (after training) \"\"\"\n",
    "def generate(checkpoint_name, checkpoint_index, config, source, output, skip_dummy):\n",
    "    \"\"\" Generate fingerprints from a saved checkpoint.\n",
    "\n",
    "    ex) python run.py generate CHECKPOINT_NAME\n",
    "\n",
    "    With custom config: \\b\\n\n",
    "        python run.py generate CHECKPOINT_NAME -c CONFIG_NAME\n",
    "\n",
    "    • If CHECKPOINT_INDEX is not specified, the latest checkpoint in the OUTPUT_ROOT_DIR will be loaded.\n",
    "    • The default value for the fingerprinting source is [TEST_DUMMY_DB] and [TEST_QUERY_DB] specified in config file.\n",
    "\n",
    "    \"\"\"\n",
    "    from model.utils.config_gpu_memory_lim import allow_gpu_memory_growth\n",
    "    from model.generate import generate_fingerprint\n",
    "\n",
    "    cfg = load_config(config)\n",
    "    allow_gpu_memory_growth()\n",
    "    generate_fingerprint(cfg, checkpoint_name, checkpoint_index, source, output, skip_dummy)\n",
    "\n",
    "def evaluate(checkpoint_name, checkpoint_index, config, index_type,\n",
    "             test_seq_len, test_ids, nogpu):\n",
    "    \"\"\" Search and evalutation.\n",
    "\n",
    "    ex) python run.py evaluate CHECKPOINT_NAME CHECKPOINT_INDEX\n",
    "\n",
    "    With options: \\b\\n\n",
    "\n",
    "    ex) python run.py evaluate CHECKPOINT_NAME CHEKPOINT_INDEX -i ivfpq -t 3000 --nogpu\n",
    "\n",
    "\n",
    "    • Currently, the 'evaluate' command does not reference any information other\n",
    "    than the output log directory from the config file.\n",
    "    \"\"\"\n",
    "    from eval.eval_faiss import eval_faiss\n",
    "\n",
    "    cfg = load_config(config)\n",
    "    emb_dir = cfg['DIR']['OUTPUT_ROOT_DIR'] + checkpoint_name + '/' + \\\n",
    "        str(checkpoint_index) + '/'\n",
    "\n",
    "    if nogpu:\n",
    "        eval_faiss([emb_dir, \"--index_type\", index_type, \"--test_seq_len\",\n",
    "                    test_seq_len, \"--test_ids\", test_ids, \"--nogpu\"])\n",
    "    else:\n",
    "        eval_faiss([emb_dir, \"--index_type\", index_type, \"--test_seq_len\",\n",
    "                    test_seq_len, \"--test_ids\", test_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name:str = \"CHECKPOINT\"   # string\n",
    "checkpoint_index:int = None  # int\n",
    "config:str = \"default\"       # string 'default'\n",
    "index_type:str = 'IVFPQ'  # {'L2', 'IVF', 'IVFPQ', \" + \"'IVFPQ-RR', 'IVFPQ-ONDISK', HNSW'}\"\n",
    "test_seq_len:str =  '11'   # string '1 3 5 9 11 19' segundos \n",
    "test_ids:str = \"icassp\"      # string 'icassp'\n",
    "nogpu:bool = False         # False or True\n",
    "max_epoch:int = 5     # int"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
