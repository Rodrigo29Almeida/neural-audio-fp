{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 15:16:18.619453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Lambda, Permute\n",
    "from kapre.time_frequency import STFT, Magnitude, ApplyFilterbank\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from model.fp.melspec.melspectrogram import Melspec_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from kapre import backend\n",
    "from kapre.backend import _CH_FIRST_STR, _CH_LAST_STR, _CH_DEFAULT_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model.dataset import Dataset\n",
    "#from model.fp.melspec.melspectrogram import get_melspec_layer\n",
    "from model.fp.specaug_chain.specaug_chain import get_specaug_chain_layer\n",
    "from model.fp.nnfp import get_fingerprinter\n",
    "from model.fp.NTxent_loss_single_gpu import NTxentLoss\n",
    "from model.fp.online_triplet_loss import OnlineTripletLoss\n",
    "from model.fp.lamb_optimizer import LAMB\n",
    "from model.utils.experiment_helper import ExperimentHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kapre.time_frequency import ApplyFilterbank\n",
    "\n",
    "class CustomApplyFilterbank(ApplyFilterbank):\n",
    "    def __init__(self, type, filterbank_kwargs, data_format='default', **kwargs):\n",
    "        super(CustomApplyFilterbank, self).__init__(type, filterbank_kwargs, data_format, **kwargs)\n",
    "\n",
    "        # tipo de Banco de filtros\n",
    "        if type == 'tri':\n",
    "            self.filterbank = self.filterbank_triangular_log(**filterbank_kwargs)\n",
    "\n",
    "        if data_format == _CH_DEFAULT_STR:\n",
    "            self.data_format = K.image_data_format()\n",
    "        else:\n",
    "            self.data_format = data_format\n",
    "\n",
    "        if self.data_format == _CH_FIRST_STR:\n",
    "            self.freq_axis = 3\n",
    "        else:\n",
    "            self.freq_axis = 2\n",
    "\n",
    "    def filterbank_triangular_log(self, sample_rate, n_fft):\n",
    "        # Com o objetivo de ter 256 giltros e 8000 Hz na frequência de amostragem, teve-se de optar por Nfft de 2048, o que resulta em 54.4024 filtros por oitava, e numa frequência mínima de 151.3483 Hz.\n",
    "        # Sendo assim, o Nfpo será 60, 5*12, e a frequência do último filtro, f256, será Si7 = 3951.066410048992 Hz. Resultando numa frequência máxima de 3996.975590329487 Hz.\n",
    "        # Com isto, obtem-se pelo menos um bin em cada filtro, visto que f0*(2^(2/Nfpo)-1) = 4.7979 > 8000/2048 = 3.9062. Para uma Nfft de 1024, não era certo que obte-se pelo menos um bin por filtro.\n",
    "\n",
    "        n_fft=2048\n",
    "        sample_rate=8000\n",
    "        Nfpo=60 #=5*12\n",
    "        Nb =256\n",
    "\n",
    "        #Cálculo da fmin e fmax\n",
    "        f256 = 440*2.**(38/12) # Si7 = 3951.066410048992 Hz;\n",
    "        f0=f256/2**(256/Nfpo) # fmin, 205.2672581380976 Hz\n",
    "        fmax = f0*2**(257/Nfpo) # fmax, 3996.975590329487 Hz\n",
    "\n",
    "        #Depois disto, dá bins em todos os fitros. Ver a linha 24 do getOctaveFilterBanck2.m\n",
    "\n",
    "        i=np.arange(1,Nb+1, dtype=float)\n",
    "        k=np.arange(n_fft//2+1)\n",
    "        f=k*sample_rate/n_fft\n",
    "\n",
    "        fcf = f0 * 2.**(i/Nfpo) #3905.68454168\n",
    "\n",
    "        fi = np.concatenate(([f0], fcf, [fmax])) #fi =[f0, fcf, fmax] \n",
    "\n",
    "        # Construct the output matrix\n",
    "        H = np.zeros((Nb, n_fft // 2 + 1))\n",
    "\n",
    "        #for i in range(n_filters), com isto são 256\n",
    "        for j in range(Nb):\n",
    "            fLow = fi[j] \n",
    "            fmid = fi[j+1] \n",
    "            fUpp = fi[j+2]\n",
    "\n",
    "            H[j, :] = ((f - fLow) / (fmid - fLow)) * ((f > fLow) & (f <= fmid)) + \\\n",
    "                            ((f - fUpp) / (fmid - fUpp)) * ((f > fmid) & (f <= fUpp))\n",
    "            \n",
    "\n",
    "        H /= np.sum(H, axis=1, keepdims=True) # : A matriz é normalizada ao longo do eixo 1 (linhas), dividindo cada valor pela soma dos valores na respectiva linha. Isto garante que a soma de cada linha seja igual a 1.65\n",
    "        \n",
    "        return tf.convert_to_tensor(H.T, dtype=tf.float32)\n",
    "        #return tf.convert_to_tensor(H.T)\n",
    "    \n",
    "\n",
    "    def call(self, x):\n",
    "        tf.print(f\"self.filterbank_shape={self.filterbank.shape}\")\n",
    "        output = tf.tensordot(x, self.filterbank, axes=(self.freq_axis, 0))\n",
    "        \n",
    "        if self.data_format == _CH_LAST_STR:\n",
    "            output = tf.transpose(output, (0, 1, 3, 2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class triBFMelspecLayer(Melspec_layer):\n",
    "    def __init__(self,\n",
    "                input_shape=(1, 8000),\n",
    "                segment_norm=False,\n",
    "                n_fft=2048,\n",
    "                stft_hop=192,\n",
    "                fs=8000,\n",
    "                dur=1.,\n",
    "                f_min=300.,\n",
    "                f_max=4000.,\n",
    "                amin=1e-10, # minimum amp.\n",
    "                dynamic_range=80.,\n",
    "                name='Mel-spectrogram',\n",
    "                trainable=False,\n",
    "                **kwargs\n",
    "                ):\n",
    "        super(triBFMelspecLayer, self).__init__(name=name, trainable=False, **kwargs)\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.stft_hop = stft_hop\n",
    "        self.amin = amin\n",
    "        self.dynamic_range = dynamic_range\n",
    "        self.segment_norm = segment_norm\n",
    "\n",
    "        self.tri_fb_kwargs = {\n",
    "            'sample_rate': fs,\n",
    "            'n_fft': n_fft,\n",
    "            }\n",
    "        \n",
    "        # Construct log-power Mel-spec layer\n",
    "        self.mm = self.construct_melspec_layer(input_shape, name)\n",
    "\n",
    "        # Permute layer\n",
    "        self.p = tf.keras.Sequential(name='Permute')\n",
    "        self.p.add(Permute((3, 2, 1), input_shape=self.m.output_shape[1:]))\n",
    "        \n",
    "        super(triBFMelspecLayer, self).build((None, input_shape[0], input_shape[1]))\n",
    "        \n",
    "    def construct_melspec_layer(self, input_shape, name):\n",
    "        mm = tf.keras.Sequential(name=name)\n",
    "        mm.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "        mm.add(self.pad_layer)\n",
    "        mm.add(\n",
    "            STFT(\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.stft_hop,\n",
    "                pad_begin=False, # We do not use Kapre's padding, due to the @tf.function compatiability\n",
    "                pad_end=False, # We do not use Kapre's padding, due to the @tf.function compatiability\n",
    "                input_data_format='channels_first',\n",
    "                output_data_format='channels_first')\n",
    "            )\n",
    "        mm.add(\n",
    "            Magnitude()\n",
    "            )\n",
    "        mm.add(\n",
    "            CustomApplyFilterbank(type='tri',\n",
    "                            filterbank_kwargs=self.tri_fb_kwargs,\n",
    "                            data_format='channels_first'\n",
    "                            )\n",
    "            )\n",
    "        return mm\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.mm(x) + 0.06\n",
    "        #x = tf.sqrt(x)\n",
    "        \n",
    "        x = tf.math.log(tf.maximum(x, self.amin)) / math.log(10)\n",
    "        x = x - tf.reduce_max(x)\n",
    "        x = tf.maximum(x, -1 * self.dynamic_range)\n",
    "        if self.segment_norm:\n",
    "            x = (x - tf.reduce_min(x) / 2) / tf.abs(tf.reduce_min(x) / 2 + 1e-10)\n",
    "        return self.p(x) # Permute((3,2,1))\n",
    "\n",
    "    \n",
    "\n",
    "# Função auxiliar para criar a camada Mel-spectrogram usando a nova classe\n",
    "def get_triBF_melspec_layer(cfg, trainable=False, new_param=None):\n",
    "    fs = 8000\n",
    "    dur = 1.\n",
    "    n_fft = 2048\n",
    "    stft_hop = 192\n",
    "    segment_norm = False\n",
    "\n",
    "    input_shape = (1, int(fs * dur))\n",
    "\n",
    "    l = triBFMelspecLayer(input_shape=input_shape,\n",
    "                            segment_norm=segment_norm,\n",
    "                            n_fft=n_fft,\n",
    "                            stft_hop=stft_hop,\n",
    "                            fs=fs,\n",
    "                            dur=dur)\n",
    "    l.trainable = trainable\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_triBF_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_specaug: spec-augmentation layer.\n",
    "    m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "    assert(m_specaug.bypass==False) # Detachable by setting m_specaug.bypass.\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    return m_pre, m_specaug, m_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import click\n",
    "import yaml\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "def load_config(config_fname):\n",
    "    config_filepath = './config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ./config/default.yaml\n"
     ]
    }
   ],
   "source": [
    "config = \"default\"\n",
    "cfg = load_config(config)\n",
    "checkpoint_name = \"Checks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataset = Dataset(cfg)\n",
    "\n",
    "# Build models.\n",
    "m_pre, m_specaug, m_fp = build_fp(cfg)\n",
    "\n",
    "# Learning schedule\n",
    "total_nsteps = cfg['TRAIN']['MAX_EPOCH'] * len(dataset.get_train_ds())\n",
    "if cfg['TRAIN']['LR_SCHEDULE'].upper() == 'COS':\n",
    "    lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "        initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "        decay_steps=total_nsteps,\n",
    "        alpha=1e-06)\n",
    "elif cfg['TRAIN']['LR_SCHEDULE'].upper() == 'COS-RESTART':\n",
    "    lr_schedule = tf.keras.experimental.CosineDecayRestarts(\n",
    "        initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "        first_decay_steps=int(total_nsteps * 0.1),\n",
    "        num_periods=0.5,\n",
    "        alpha=2e-06)\n",
    "else:\n",
    "    lr_schedule = float(cfg['TRAIN']['LR'])\n",
    "\n",
    "# Optimizer\n",
    "if cfg['TRAIN']['OPTIMIZER'].upper() == 'LAMB':\n",
    "    opt = LAMB(learning_rate=lr_schedule)\n",
    "elif cfg['TRAIN']['OPTIMIZER'].upper() == 'ADAM':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "else:\n",
    "    raise NotImplementedError(cfg['TRAIN']['OPTIMIZER'])\n",
    "\n",
    "# Experiment helper: see utils.experiment_helper.py for details.\n",
    "helper = ExperimentHelper(\n",
    "    checkpoint_name=checkpoint_name,\n",
    "    optimizer=opt,\n",
    "    model_to_checkpoint=m_fp,\n",
    "    cfg=cfg)\n",
    "\n",
    "# Loss objects\n",
    "if cfg['LOSS']['LOSS_MODE'].upper() == 'NTXENT': # Default\n",
    "    loss_obj_train = NTxentLoss(\n",
    "        n_org=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "        n_rep=cfg['BSZ']['TR_BATCH_SZ'] - cfg['BSZ']['TR_N_ANCHOR'],\n",
    "        tau=cfg['LOSS']['TAU'])\n",
    "    loss_obj_val = NTxentLoss(\n",
    "        n_org=cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "        n_rep=cfg['BSZ']['VAL_BATCH_SZ'] - cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "        tau=cfg['LOSS']['TAU'])\n",
    "elif cfg['LOSS']['LOSS_MODE'].upper() == 'ONLINE-TRIPLET': # Now-playing\n",
    "    loss_obj_train = OnlineTripletLoss(\n",
    "        bsz=cfg['BSZ']['TR_BATCH_SZ'],\n",
    "        n_anchor=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "        mode = 'semi-hard',\n",
    "        margin=cfg['LOSS']['MARGIN'])\n",
    "    loss_obj_val = OnlineTripletLoss(\n",
    "        bsz=cfg['BSZ']['VAL_BATCH_SZ'],\n",
    "        n_anchor=cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "        mode = 'all', # use 'all' mode for validation\n",
    "        margin=0.)\n",
    "else:\n",
    "    raise NotImplementedError(cfg['LOSS']['LOSS_MODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.get_train_ds(0)\n",
    "enq = tf.keras.utils.OrderedEnqueuer(\n",
    "        train_ds, use_multiprocessing=True, shuffle=train_ds.shuffle)\n",
    "enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "        max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(enq.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_anchors = len(X[0])\n",
    "X = tf.concat(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
