{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import click\n",
    "import yaml\n",
    "import numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" trainer.py \"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Progbar\n",
    "import tensorflow.keras as K\n",
    "from model.dataset import Dataset\n",
    "from model.fp.melspec.melspectrogram import get_melspec_layer\n",
    "from model.fp.specaug_chain.specaug_chain import get_specaug_chain_layer\n",
    "from model.fp.nnfp import get_fingerprinter\n",
    "from model.fp.NTxent_loss_single_gpu import NTxentLoss\n",
    "from model.fp.online_triplet_loss import OnlineTripletLoss\n",
    "from model.fp.lamb_optimizer import LAMB\n",
    "from model.utils.experiment_helper import ExperimentHelper\n",
    "from model.utils.mini_search_subroutines import mini_search_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_fname):\n",
    "    config_filepath = './config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def update_config(cfg, key1: str, key2: str, val):\n",
    "    cfg[key1][key2] = val\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def print_config(cfg):\n",
    "    os.system(\"\")\n",
    "    print('\\033[36m' + yaml.dump(cfg, indent=4, width=120, sort_keys=False) +\n",
    "          '\\033[0m')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name:str = \"Checks_test_generate\"   # string\n",
    "checkpoint_index:int = 100  # int\n",
    "config:str = \"default\"       # string 'default'\n",
    "source_root_dir:str = '/mnt/dataset/public/Fingerprinting/neural-audio-fp-dataset/music/test-dummy-db-100k-full/'\n",
    "output_root_dir:str = './logs/emb/'\n",
    "skip_dummy:bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ./config/default.yaml\n"
     ]
    }
   ],
   "source": [
    "from model.utils.config_gpu_memory_lim import allow_gpu_memory_growth\n",
    "from model.generate import generate_fingerprint\n",
    "\n",
    "cfg = load_config(config)\n",
    "allow_gpu_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_fingerprint(cfg, checkpoint_name, checkpoint_index, ..., ..., skip_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    return m_pre, m_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_root_dir, checkpoint_name, checkpoint_index,\n",
    "                    m_fp):\n",
    "    \"\"\" Load a trained fingerprinter \"\"\"\n",
    "    # Create checkpoint\n",
    "    checkpoint = tf.train.Checkpoint(model=m_fp)\n",
    "    checkpoint_dir = checkpoint_root_dir + f'/{checkpoint_name}/'\n",
    "    c_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir,\n",
    "                                           max_to_keep=None)\n",
    "\n",
    "    # Load\n",
    "    if checkpoint_index == None:\n",
    "        tf.print(\"\\x1b[1;32mArgument 'checkpoint_index' was not specified.\\x1b[0m\")\n",
    "        tf.print('\\x1b[1;32mSearching for the latest checkpoint...\\x1b[0m')\n",
    "        latest_checkpoint = c_manager.latest_checkpoint\n",
    "        if latest_checkpoint:\n",
    "            checkpoint_index = int(latest_checkpoint.split(sep='ckpt-')[-1])\n",
    "            status = checkpoint.restore(latest_checkpoint)\n",
    "            status.expect_partial()\n",
    "            tf.print(f'---Restored from {c_manager.latest_checkpoint}---')\n",
    "        else:\n",
    "            raise FileNotFoundError(f'Cannot find checkpoint in {checkpoint_dir}')\n",
    "    else:\n",
    "        checkpoint_fpath = checkpoint_dir + 'ckpt-' + str(checkpoint_index)\n",
    "        status = checkpoint.restore(checkpoint_fpath) # Let TF to handle error cases.\n",
    "        status.expect_partial()\n",
    "        tf.print(f'---Restored from {checkpoint_fpath}---')\n",
    "    return checkpoint_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Restored from ./logs/checkpoint//Checks_test_generate/ckpt-100---\n"
     ]
    }
   ],
   "source": [
    "# Build and load checkpoint\n",
    "m_pre, m_fp = build_fp(cfg)\n",
    "checkpoint_root_dir = cfg['DIR']['LOG_ROOT_DIR'] + 'checkpoint/'\n",
    "checkpoint_index = load_checkpoint(checkpoint_root_dir, checkpoint_name,\n",
    "                                    checkpoint_index, m_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_source(cfg, source_root_dir, skip_dummy):\n",
    "    dataset = Dataset(cfg)\n",
    "    ds = dict()\n",
    "    if skip_dummy:\n",
    "        tf.print(\"Excluding \\033[33m'dummy_db'\\033[0m from source.\")\n",
    "        pass\n",
    "    else:\n",
    "        ds['dummy_db'] = dataset.get_test_dummy_db_ds()\n",
    "\n",
    "    if dataset.datasel_test_query_db in ['unseen_icassp', 'unseen_syn']:\n",
    "        ds['query'], ds['db'] = dataset.get_test_query_db_ds()\n",
    "    else:\n",
    "        raise ValueError(dataset.datasel_test_query_db)\n",
    "\n",
    "    tf.print(f'\\x1b[1;32mData source: {ds.keys()}\\x1b[0m',\n",
    "             f'{dataset.datasel_test_query_db}')\n",
    "    \n",
    "    print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevent_overwrite(key, target_path):\n",
    "    if (key == 'dummy_db') & os.path.exists(target_path):\n",
    "        answer = input(f'{target_path} exists. Will you overwrite (y/N)?')\n",
    "        if answer.lower() not in ['y', 'yes']: sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mData source: dict_keys(['dummy_db', 'query', 'db'])\u001b[0m unseen_icassp\n",
      "{'dummy_db': <model.utils.dataloader_keras.genUnbalSequence object at 0x73123b6a00d0>, 'query': <model.utils.dataloader_keras.genUnbalSequence object at 0x731026791f50>, 'db': <model.utils.dataloader_keras.genUnbalSequence object at 0x73123b262950>}\n"
     ]
    }
   ],
   "source": [
    "# Get data source\n",
    "\"\"\" ds = {'key1': <Dataset>, 'key2': <Dataset>, ...} \"\"\"\n",
    "ds = get_data_source(cfg, ..., skip_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dummy_db': <model.utils.dataloader_keras.genUnbalSequence at 0x73123b6a00d0>,\n",
       " 'query': <model.utils.dataloader_keras.genUnbalSequence at 0x731026791f50>,\n",
       " 'db': <model.utils.dataloader_keras.genUnbalSequence at 0x73123b262950>}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dummy_db', 'query', 'db'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53754198\n",
      "29500\n",
      "29500\n"
     ]
    }
   ],
   "source": [
    "for key in ds.keys():\n",
    "    print(ds[key].n_samples)\n",
    "    n_items=ds[key].n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29500, 128)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = cfg['MODEL']['EMB_SZ']\n",
    "arr_shape = (n_items, dim)\n",
    "arr_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directory\n",
    "if output_root_dir:\n",
    "    output_root_dir = output_root_dir + f'/{checkpoint_name}/{checkpoint_index}/'\n",
    "else:\n",
    "    output_root_dir = cfg['DIR']['OUTPUT_ROOT_DIR'] + \\\n",
    "        f'/{checkpoint_name}/{checkpoint_index}/'\n",
    "os.makedirs(output_root_dir, exist_ok=True)\n",
    "if not skip_dummy:\n",
    "    prevent_overwrite('dummy_db', f'{output_root_dir}/dummy_db.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(X, m_pre, m_fp):\n",
    "    \"\"\" Test step used for generating fingerprint \"\"\"\n",
    "    # X is not (Xa, Xp) here. The second element is reduced now.\n",
    "    m_fp.trainable = False\n",
    "    return m_fp(m_pre(X))  # (BSZ, Dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating fingerprint from \u001b[1;32m'query'\u001b[0m bsz=125, 29500 items, d=128 ===\n"
     ]
    }
   ],
   "source": [
    "sz_check = dict() # for warning message\n",
    "key = 'query'\n",
    "bsz = int(cfg['BSZ']['TS_BATCH_SZ'])  # Do not use ds.bsz here.\n",
    "# n_items = len(ds[key]) * bsz\n",
    "n_items =  ds['query'].n_samples\n",
    "dim = cfg['MODEL']['EMB_SZ']\n",
    "\n",
    "assert n_items > 0\n",
    "arr_shape = (n_items, dim)\n",
    "arr = np.memmap(f'{output_root_dir}/{key}.mm',\n",
    "                dtype='float32',\n",
    "                mode='w+',\n",
    "                shape=arr_shape)\n",
    "np.save(f'{output_root_dir}/{key}_shape.npy', arr_shape)\n",
    "\n",
    "# Fingerprinting loop\n",
    "tf.print(\n",
    "    f\"=== Generating fingerprint from \\x1b[1;32m'{key}'\\x1b[0m \" +\n",
    "    f\"bsz={bsz}, {n_items} items, d={dim}\"+ \" ===\")\n",
    "progbar = Progbar(len(ds[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parallelism to speed up preprocessing------------------------- \"\"\"\n",
    "enq = tf.keras.utils.OrderedEnqueuer(ds[key],\n",
    "                                        use_multiprocessing=True,\n",
    "                                        shuffle=False)\n",
    "enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "            max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 18s 57ms/step\n"
     ]
    }
   ],
   "source": [
    "while i < len(enq.sequence):\n",
    "    progbar.update(i)\n",
    "    X, _ = next(enq.get())\n",
    "    #print(f\"\\n\\nX{i}={X}\\n\\n\") #matriz\n",
    "    emb = test_step(X, m_pre, m_fp)\n",
    "    #print(f\"\\n\\nemb{i}={emb}\\n\\n\") #matriz\n",
    "    arr[i * bsz:(i + 1) * bsz, :] = emb.numpy() # Writing on disk.\n",
    "    #print(f\"\\n\\narr[i * bsz:(i + 1) * bsz, :]{i}={arr[i * bsz:(i + 1) * bsz, :]}\\n\\n\") #matriz\n",
    "    i += 1\n",
    "progbar.update(i, finalize=True)\n",
    "enq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Succesfully stored 29500 fingerprint to ./logs/emb//Checks_test_generate/100/ ===\n",
      "sz_check=29500\n",
      "len(arr)=29500\n"
     ]
    }
   ],
   "source": [
    "tf.print(f'=== Succesfully stored {arr_shape[0]} fingerprint to {output_root_dir} ===')\n",
    "sz_check[key] = len(arr)\n",
    "print(f\"sz_check={sz_check[key]}\")\n",
    "print(f\"len(arr)={len(arr)}\")\n",
    "arr.flush(); del(arr) # Close memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "sz_check = dict() # for warning message\n",
    "for key in ds.keys():\n",
    "    bsz = int(cfg['BSZ']['TS_BATCH_SZ'])  # Do not use ds.bsz here.\n",
    "    # n_items = len(ds[key]) * bsz\n",
    "    n_items = ds[key].n_samples\n",
    "    dim = cfg['MODEL']['EMB_SZ']\n",
    "    \"\"\"\n",
    "    Why use \"memmap\"?\n",
    "\n",
    "    • First, we need to store a huge uncompressed embedding vectors until\n",
    "        constructing a compressed DB with IVF-PQ (using FAISS). Handling a\n",
    "        huge ndarray is not a memory-safe way: \"memmap\" consume 0 memory.\n",
    "\n",
    "    • Second, Faiss-GPU does not support reconstruction of DB from\n",
    "        compressed DB (index). In eval/eval_faiss.py, we need uncompressed\n",
    "        vectors to calaulate sequence-level matching score. The created\n",
    "        \"memmap\" will be reused at that point.\n",
    "\n",
    "    Reference:\n",
    "        https://numpy.org/doc/stable/reference/generated/numpy.memmap.html\n",
    "\n",
    "    \"\"\"\n",
    "    # Create memmap, and save shapes\n",
    "    assert n_items > 0\n",
    "    arr_shape = (n_items, dim)\n",
    "    arr = np.memmap(f'{output_root_dir}/{key}.mm',\n",
    "                    dtype='float32',\n",
    "                    mode='w+',\n",
    "                    shape=arr_shape)\n",
    "    np.save(f'{output_root_dir}/{key}_shape.npy', arr_shape)\n",
    "\n",
    "    # Fingerprinting loop\n",
    "    tf.print(\n",
    "        f\"=== Generating fingerprint from \\x1b[1;32m'{key}'\\x1b[0m \" +\n",
    "        f\"bsz={bsz}, {n_items} items, d={dim}\"+ \" ===\")\n",
    "    progbar = Progbar(len(ds[key]))\n",
    "\n",
    "    \"\"\" Parallelism to speed up preprocessing------------------------- \"\"\"\n",
    "    enq = tf.keras.utils.OrderedEnqueuer(ds[key],\n",
    "                                            use_multiprocessing=True,\n",
    "                                            shuffle=False)\n",
    "    enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "                max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "    i = 0\n",
    "    while i < len(enq.sequence):\n",
    "        progbar.update(i)\n",
    "        X, _ = next(enq.get())\n",
    "        emb = test_step(X, m_pre, m_fp)\n",
    "        arr[i * bsz:(i + 1) * bsz, :] = emb.numpy() # Writing on disk.\n",
    "        i += 1\n",
    "    progbar.update(i, finalize=True)\n",
    "    enq.stop()\n",
    "    \"\"\" End of Parallelism-------------------------------------------- \"\"\"\n",
    "\n",
    "    tf.print(f'=== Succesfully stored {arr_shape[0]} fingerprint to {output_root_dir} ===')\n",
    "    sz_check[key] = len(arr)\n",
    "    arr.flush(); del(arr) # Close memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fingerprint(cfg,\n",
    "                         checkpoint_name,\n",
    "                         checkpoint_index,\n",
    "                         source_root_dir,\n",
    "                         output_root_dir,\n",
    "                         skip_dummy):\n",
    "    \"\"\"\n",
    "    After run, the output (generated fingerprints) directory will be:\n",
    "      .\n",
    "      └──logs\n",
    "         └── emb\n",
    "             └── CHECKPOINT_NAME\n",
    "                 └── CHECKPOINT_INDEX\n",
    "                     ├── db.mm\n",
    "                     ├── db_shape.npy\n",
    "                     ├── dummy_db.mm\n",
    "                     ├── dummy_db_shape.npy\n",
    "                     ├── query.mm\n",
    "                     └── query_shape.npy\n",
    "    \"\"\"\n",
    "    # Build and load checkpoint\n",
    "    m_pre, m_fp = build_fp(cfg)\n",
    "    checkpoint_root_dir = cfg['DIR']['LOG_ROOT_DIR'] + 'checkpoint/'\n",
    "    checkpoint_index = load_checkpoint(checkpoint_root_dir, checkpoint_name,\n",
    "                                       checkpoint_index, m_fp)\n",
    "\n",
    "    # Get data source\n",
    "    \"\"\" ds = {'key1': <Dataset>, 'key2': <Dataset>, ...} \"\"\"\n",
    "    ds = get_data_source(cfg, source_root_dir, skip_dummy)\n",
    "\n",
    "    # Make output directory\n",
    "    if output_root_dir:\n",
    "        output_root_dir = output_root_dir + f'/{checkpoint_name}/{checkpoint_index}/'\n",
    "    else:\n",
    "        output_root_dir = cfg['DIR']['OUTPUT_ROOT_DIR'] + \\\n",
    "            f'/{checkpoint_name}/{checkpoint_index}/'\n",
    "    os.makedirs(output_root_dir, exist_ok=True)\n",
    "    if not skip_dummy:\n",
    "        prevent_overwrite('dummy_db', f'{output_root_dir}/dummy_db.mm')\n",
    "\n",
    "    # Generate\n",
    "    sz_check = dict() # for warning message\n",
    "    for key in ds.keys():\n",
    "        bsz = int(cfg['BSZ']['TS_BATCH_SZ'])  # Do not use ds.bsz here.\n",
    "        # n_items = len(ds[key]) * bsz\n",
    "        n_items = ds[key].n_samples\n",
    "        dim = cfg['MODEL']['EMB_SZ']\n",
    "        \"\"\"\n",
    "        Why use \"memmap\"?\n",
    "\n",
    "        • First, we need to store a huge uncompressed embedding vectors until\n",
    "          constructing a compressed DB with IVF-PQ (using FAISS). Handling a\n",
    "          huge ndarray is not a memory-safe way: \"memmap\" consume 0 memory.\n",
    "\n",
    "        • Second, Faiss-GPU does not support reconstruction of DB from\n",
    "          compressed DB (index). In eval/eval_faiss.py, we need uncompressed\n",
    "          vectors to calaulate sequence-level matching score. The created\n",
    "          \"memmap\" will be reused at that point.\n",
    "\n",
    "        Reference:\n",
    "            https://numpy.org/doc/stable/reference/generated/numpy.memmap.html\n",
    "\n",
    "        \"\"\"\n",
    "        # Create memmap, and save shapes\n",
    "        assert n_items > 0\n",
    "        arr_shape = (n_items, dim)\n",
    "        arr = np.memmap(f'{output_root_dir}/{key}.mm',\n",
    "                        dtype='float32',\n",
    "                        mode='w+',\n",
    "                        shape=arr_shape)\n",
    "        np.save(f'{output_root_dir}/{key}_shape.npy', arr_shape)\n",
    "\n",
    "        # Fingerprinting loop\n",
    "        tf.print(\n",
    "            f\"=== Generating fingerprint from \\x1b[1;32m'{key}'\\x1b[0m \" +\n",
    "            f\"bsz={bsz}, {n_items} items, d={dim}\"+ \" ===\")\n",
    "        progbar = Progbar(len(ds[key]))\n",
    "\n",
    "        \"\"\" Parallelism to speed up preprocessing------------------------- \"\"\"\n",
    "        enq = tf.keras.utils.OrderedEnqueuer(ds[key],\n",
    "                                              use_multiprocessing=True,\n",
    "                                              shuffle=False)\n",
    "        enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "                  max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "        i = 0\n",
    "        while i < len(enq.sequence):\n",
    "            progbar.update(i)\n",
    "            X, _ = next(enq.get())\n",
    "            emb = test_step(X, m_pre, m_fp)\n",
    "            arr[i * bsz:(i + 1) * bsz, :] = emb.numpy() # Writing on disk.\n",
    "            i += 1\n",
    "        progbar.update(i, finalize=True)\n",
    "        enq.stop()\n",
    "        \"\"\" End of Parallelism-------------------------------------------- \"\"\"\n",
    "\n",
    "        tf.print(f'=== Succesfully stored {arr_shape[0]} fingerprint to {output_root_dir} ===')\n",
    "        sz_check[key] = len(arr)\n",
    "        arr.flush(); del(arr) # Close memmap\n",
    "\n",
    "    if 'custom_source' in ds.keys():\n",
    "        pass;\n",
    "    elif sz_check['db'] != sz_check['query']:\n",
    "        print(\"\\033[93mWarning: 'db' and 'query' size does not match. This can cause a problem in evaluataion stage.\\033[0m\")\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
