{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 00:46:05.266821: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import h5py\n",
    "import faiss\n",
    "import click\n",
    "import curses\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "from model_RA.fp_RA.melspec.melspectrogram_RA import get_melspec_layer\n",
    "from model_RA.fp_RA.nnfp import get_fingerprinter\n",
    "from model_RA.utils.dataloader_keras import genUnbalSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construção do BD Vetorial\n",
    "\n",
    "1) Carregar os vetores embedded\n",
    "2) Criar a instância do Faiss\n",
    "3) Carregar o Faiss com os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leitura do Modelo Neural\n",
    "\n",
    "1) Carrega a classe do modelo\n",
    "2) ler o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predição\n",
    "\n",
    "1) Receber o dado (áudio query)\n",
    "2) calcular o embedded - model.predict\n",
    "3) buscar o índice no BD vetorial\n",
    "4) recuperar os metadados associados ao índice (nome da música, etc)\n",
    "5) apresentar ao cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construção do BD Vetorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_data(source_dir):\n",
    "    h5Files = sorted(glob.glob(source_dir + '**/*.h5', recursive=True))\n",
    "\n",
    "    embs_count = 0\n",
    "    embs_info = []\n",
    "    embs = []\n",
    "    music_names = []\n",
    "\n",
    "    for i in range(len(h5Files[:29998])):\n",
    "        with h5py.File(h5Files[i], \"r\") as f:\n",
    "            #print(i)\n",
    "            base_name = os.path.splitext(os.path.basename(h5Files[i]))[0]\n",
    "            #primeiro objeto é o que contém os embeddings\n",
    "            a_group_key = list(f.keys())[0]\n",
    "\n",
    "            #Extração dos embs como um array\n",
    "            ds_arr = f[a_group_key][()]  # returns as a numpy array\n",
    "            #print(ds_arr.shape)\n",
    "            embeddings = np.squeeze(ds_arr, axis=1)\n",
    "            #print(embeddings.shape)\n",
    "            embs.append(embeddings) #Guarda na lista os embs\n",
    "            #arrayEmb = ds_arr\n",
    "\n",
    "            embs_count += len(ds_arr) #conta quantos embs tem o vetor\n",
    "            embs_info.append([i, base_name, embs_count]) #guarda numa lista o número de vetores até o momento.\n",
    "            # embs_info = [indice, file_name, n_segs]\n",
    "\n",
    "            music_names.extend([base_name] * len(ds_arr))\n",
    "            \n",
    "            f.close()\n",
    "    return embs, embs_info, music_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memmap_data(source_dir,\n",
    "                     fname,\n",
    "                     append_extra_length=None,\n",
    "                     shape_only=False,\n",
    "                     display=True):\n",
    "\n",
    "    path_shape = source_dir + fname + '_shape.npy'\n",
    "    path_data = source_dir + fname + '.mm'\n",
    "    data_shape = np.load(path_shape)\n",
    "    if shape_only:\n",
    "        return data_shape\n",
    "\n",
    "    if append_extra_length:\n",
    "        data_shape[0] += append_extra_length\n",
    "        data = np.memmap(path_data, dtype='float32', mode='r+',\n",
    "                         shape=(data_shape[0], data_shape[1]))\n",
    "    else:\n",
    "        data = np.memmap(path_data, dtype='float32', mode='r',\n",
    "                         shape=(data_shape[0], data_shape[1]))\n",
    "    if display:\n",
    "        print(f'Load {data_shape[0]:,} items from \\033[32m{path_data}\\033[0m.')\n",
    "        \n",
    "    return data, data_shape\n",
    "\n",
    "\n",
    "def create_index(db_embeddings, nogpu=True, n_centroids=256, code_sz=64, nbits=8):\n",
    "    #faiss.IndexIVFPQ(quantizer, d, n_centroids, code_sz, nbits), com d=, nlist=n_centroids=50, m=code_sz=8, bits=nbits=8\n",
    "    #faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)\n",
    "    #n_centroids -> clusters\n",
    "    \n",
    "    d = db_embeddings.shape[1]  # Dim emb #len(db_embeddings[0][0][0])\n",
    "\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "\n",
    "    code_sz = 64 # power of 2\n",
    "    n_centroids = 256 #Veronoi Cells\n",
    "    nbits = 8  # nbits must be 8, 12 or 16, The dimension d should be a multiple of M.\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, n_centroids, code_sz, nbits) #Adicona clustering\n",
    "\n",
    "    # Se não usar GPU\n",
    "    if not nogpu:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "    if not index.is_trained:\n",
    "        index.train(db_embeddings)\n",
    "\n",
    "    # Adicionando os embeddings ao índice\n",
    "    index.add(db_embeddings)\n",
    "    print(f\"Foram adicionados:{index.ntotal}\")\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega os embeddings de cada música da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 53,754,198 items from \u001b[32m/mnt/dev/rodrigoalmeida/neural-audio-fp/logs/emb/CHECK_BFTRI_100/101/dummy_db.mm\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/mnt/dev/rodrigoalmeida/neural-audio-fp/logs/emb/CHECK_BFTRI_100/101/'\n",
    "dummy_db, dummy_db_shape = load_memmap_data(data_dir, 'dummy_db')\n",
    "h5Dir = '/mnt/dataset/public/Fingerprinting/Embeddings_BFTRI/dummy_db/'\n",
    "h5Embs, embs_info, music_names = load_h5_data(h5Dir)\n",
    "embsArrayDummy=np.vstack(h5Embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria os índices, pois são adicionados os embeddings da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram adicionados:17336985\n"
     ]
    }
   ],
   "source": [
    "faiss_engine = create_index(embsArrayDummy, nogpu=True, n_centroids=256, code_sz=64, nbits=8) #dummy_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leitura do modelo Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_fname):\n",
    "    config_filepath = './config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    return m_pre, m_fp\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def predict(X, m_pre, m_fp):\n",
    "    \"\"\" \n",
    "    Test step used for mini-search-validation \n",
    "    X -> (B,1,8000)\n",
    "    \"\"\"\n",
    "    #tf.print(X)\n",
    "    feat = m_pre(X)  # (nA+nP, F, T, 1)\n",
    "    m_fp.trainable = False\n",
    "    emb_f = m_fp.front_conv(feat)  # (BSZ, Dim)\n",
    "    emb_gf = m_fp.div_enc(emb_f)\n",
    "    emb_gf = tf.math.l2_normalize(emb_gf, axis=1)\n",
    "    \n",
    "    return emb_gf # f(.), L2(f(.)), L2(g(f(.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ./config/default_RA.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x72aa4be62790>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = \"default_RA\"\n",
    "cfg = load_config(config)\n",
    "\n",
    "m_pre, m_fp = build_fp(cfg)\n",
    "\n",
    "checkpoint_root_dir:str = \"./logs/CHECK_BFTRI_100/101/\"\n",
    "checkpoint = tf.train.Checkpoint(m_fp)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_root_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predição\n",
    "### Número de indices de retorno do 'faiss search' e metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN = 1\n",
    "metadata_file = \"/mnt/dataset/public/Fingerprinting/selected_tracks.csv\"\n",
    "metadata_df = pd.read_csv(metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega áudio-query e cria o embedding do mesmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(audiofile, sr_target=8000):\n",
    "    audio, fs = librosa.load(audiofile, mono=True, sr=sr_target)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_root_dir = '/mnt/dataset/public/Fingerprinting/neural-audio-fp-dataset/music/'\n",
    "audio_dir = '/mnt/dataset/public/Fingerprinting/query_procura/000003.wav' #audio query\n",
    "\"\"\"ts_dummy_db_source_fps = sorted(\n",
    "    glob.glob(audio_dir, recursive=True))\"\"\"\n",
    "\n",
    "dur = cfg['MODEL']['DUR']\n",
    "hop = cfg['MODEL']['HOP']\n",
    "fs = cfg['MODEL']['FS']\n",
    "bsz = ts_batch_sz = cfg['BSZ']['TS_BATCH_SZ']\n",
    "\n",
    "_ts_n_anchor = ts_batch_sz\n",
    "ds = genUnbalSequence(\n",
    "    list(audio_dir),\n",
    "    ts_batch_sz,\n",
    "    _ts_n_anchor,\n",
    "    dur,\n",
    "    hop,\n",
    "    fs,\n",
    "    shuffle=False,\n",
    "    random_offset_anchor=False,\n",
    "    drop_the_last_non_full_batch=False)\n",
    "\n",
    "enq = tf.keras.utils.OrderedEnqueuer(ds,use_multiprocessing=True,shuffle=False)\n",
    "enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'], max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "\n",
    "i = 0\n",
    "emb_query_list = []\n",
    "\n",
    "while i < len(enq.sequence):\n",
    "    X, _ = next(enq.get())\n",
    "    emb = predict(X, m_pre, m_fp)\n",
    "    emb_query_list.append(emb.numpy())\n",
    "    i += 1\n",
    "enq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_query_list, está dividido em batchs de 125+125+125+98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef split_audio_into_segments(audio, dur=1.0, hop=.5, fs=8000):\\n    segment_samples = int(dur * fs) # 8000\\n    hop_samples = int(hop * fs) # 4000\\n\\n    segments = []\\n    for start in range(0, len(audio) - segment_samples + 1, hop_samples):\\n        #len(audio) = 1898580; segment_samples = 8000; hop_samples = 4000\\n        #0, 1898580 - 8000 + 1, 4000\\n\\n        #fica a faltar as residual frames, que são as ultimas que são menores que 1 segundo\\n\\n        segment = audio[start:start + segment_samples] #faz os vetores de 8000 em 8000, isto é, de 1s em 1s, com \\n\\n        if len(segment) == segment_samples:\\n            segments.append(segment)\\n\\n    return segments\\n\\n\\n\\ndur=cfg[\\'MODEL\\'][\\'DUR\\'] \\nhop=cfg[\\'MODEL\\'][\\'HOP\\'] \\nfs=cfg[\\'MODEL\\'][\\'FS\\']\\n\\naudio = get_audio(audio_dir)\\nsegments = split_audio_into_segments(audio, dur, hop, fs)\\nX_segments = tf.convert_to_tensor(segments, dtype=tf.float32)\\n\\ncheckpoint_root_dir:str = \"./logs/CHECK_BFTRI_100/101/\"\\ncheckpoint = tf.train.Checkpoint(m_fp)\\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_root_dir))\\n\\nemb = [] \\nfor i in range(len(X_segments)):\\n    X = tf.reshape(X_segments[i],(1, 1,-1)) # podia meter um append, mas pode ser uma variável auxiliar, porque só preciso do tensor para gerar o embedding\\n\\n    embedding = predict(X, m_pre, m_fp)\\n    emb.append(embedding.numpy())\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def split_audio_into_segments(audio, dur=1.0, hop=.5, fs=8000):\n",
    "    segment_samples = int(dur * fs) # 8000\n",
    "    hop_samples = int(hop * fs) # 4000\n",
    "\n",
    "    segments = []\n",
    "    for start in range(0, len(audio) - segment_samples + 1, hop_samples):\n",
    "        #len(audio) = 1898580; segment_samples = 8000; hop_samples = 4000\n",
    "        #0, 1898580 - 8000 + 1, 4000\n",
    "\n",
    "        #fica a faltar as residual frames, que são as ultimas que são menores que 1 segundo\n",
    "\n",
    "        segment = audio[start:start + segment_samples] #faz os vetores de 8000 em 8000, isto é, de 1s em 1s, com \n",
    "\n",
    "        if len(segment) == segment_samples:\n",
    "            segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "dur=cfg['MODEL']['DUR'] \n",
    "hop=cfg['MODEL']['HOP'] \n",
    "fs=cfg['MODEL']['FS']\n",
    "\n",
    "audio = get_audio(audio_dir)\n",
    "segments = split_audio_into_segments(audio, dur, hop, fs)\n",
    "X_segments = tf.convert_to_tensor(segments, dtype=tf.float32)\n",
    "\n",
    "checkpoint_root_dir:str = \"./logs/CHECK_BFTRI_100/101/\"\n",
    "checkpoint = tf.train.Checkpoint(m_fp)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_root_dir))\n",
    "\n",
    "emb = [] \n",
    "for i in range(len(X_segments)):\n",
    "    X = tf.reshape(X_segments[i],(1, 1,-1)) # podia meter um append, mas pode ser uma variável auxiliar, porque só preciso do tensor para gerar o embedding\n",
    "\n",
    "    embedding = predict(X, m_pre, m_fp)\n",
    "    emb.append(embedding.numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) carregar áudio\n",
    "#audio, fs = get_audio(audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX = tf.convert_to_tensor(audiox, dtype=tf.float32)\\nX = tf.reshape(X, (1, 1, 8000))\\n_, emb = predict(X, m_pre, m_fp)\\nemb= emb.numpy()\\n'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) gerar o embedded\n",
    "\"\"\"\n",
    "segments = split_audio_into_segments(audio, dur, hop, fs)\n",
    "X_segments = tf.convert_to_tensor(segments, dtype=tf.float32)\n",
    "\n",
    "emb = [] \n",
    "for i in range(len(X_segments)):\n",
    "    X = tf.reshape(X_segments[i],(1, 1,-1)) # podia meter um append, mas pode ser uma variável auxiliar, porque só preciso do tensor para gerar o embedding\n",
    "\n",
    "    _,embedding = predict(X, m_pre, m_fp)\n",
    "    emb.append(embedding.numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_query_array = np.vstack(emb_query_list) #emb_array[472] = emb_query_list[3][97] pois emb_query_list tem 4 batches, sendo os 3 primeiros preenchidos até 125 vetores, e o último com 98 vetores.\n",
    "#Com esta conversão passo a ter um array com todos os vetores, ou seja, os 473\n",
    "#genérico: O tem a seguinte forma emb_query_list[N_BSZ][BSZ], e quando o último não está preenchido tem o valor que entre 0 e 125. Pois, BSZ de teste = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Faiss Search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17336985\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(faiss_engine.ntotal)\n",
    "print(faiss_engine.nprobe)\n",
    "\n",
    "# c) Buscar o índice\n",
    "D, I = faiss_engine.search(emb_query_array, 1) # D: Distâncias, I: Índices dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 128)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_query_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#candidates = np.unique(I[np.where(I >= 0)])   # ignore id < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29998"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embs_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10946321])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17336985"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_info[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'033986'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_names[I[0][0]]\n",
    "#map ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10946321"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_names = np.array(music_names)\n",
    "\n",
    "map_obra = lambda idx: music_names[idx]\n",
    "\n",
    "obras_result = map_obra(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['003365' '014460' '019548' '019636' '019637' '019787' '019878' '019903'\n",
      " '019913' '019914' '020066' '020762' '021277' '021278' '021308' '022066'\n",
      " '022127' '022739' '024458' '024459' '026359' '026500' '027140' '027141'\n",
      " '027459' '027461' '028554' '031797' '031798' '032203' '032926' '033030'\n",
      " '033357' '033985' '033986' '034203' '035034' '036782' '036785' '037694'\n",
      " '038451' '038452' '038963' '038968' '039204' '040710' '040798' '040954'\n",
      " '041233' '041377' '041378' '041872' '043253' '044230' '044747' '045936'\n",
      " '046756' '047667' '048605'] [  2   1   1   4   1   1   1   1  59   3   2   1   3   1   1   1   1   3\n",
      "  18  19   2   1   2   1   7   9   3   8   2   4   1   5   1 130 128   1\n",
      "   3   1   1   1   1   1   1   1   1   1   1   1   1   5   3   3   1   1\n",
      "   1   4   1   9   1]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#obras_result\n",
    "unique_values, counts = np.unique(obras_result, return_counts=True)\n",
    "print(unique_values, counts)\n",
    "\n",
    "valor_com_mais_votos = int(unique_values[np.argmax(counts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       track_id         artist_name        track_title\n",
      "21753     33985  The Pleasure Kills  Dancing On My Bed\n"
     ]
    }
   ],
   "source": [
    "#00003\n",
    "# d) Recuperar os metadados\n",
    "data = metadata_df.loc[metadata_df[\"track_id\"] == valor_com_mais_votos]\n",
    "\n",
    "# e) Retornar ao Cliente\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "idx = 0\n",
    "for i in range(len(embs_info)):\n",
    "    if embs_info[i][2] < I[0]:\n",
    "        x=embs_info[i+1][2]\n",
    "        idx = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19091, 10946453, array([10946321]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, x, I[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_minimo = np.argmin(D)\n",
    "valor_minimo = D[indice_minimo]\n",
    "print(indice_minimo, valor_minimo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(II, return_counts=True)\n",
    "print(unique_values, counts)\n",
    "valor_com_mais_votos = unique_values[np.argmax(counts)]\n",
    "print(valor_com_mais_votos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1523,\n",
       " '003365',\n",
       " 764383,\n",
       " array([[[-0.0110972 , -0.07463508,  0.05751495, ...,  0.03888726,\n",
       "          -0.01230168,  0.10429765]],\n",
       " \n",
       "        [[-0.07340898, -0.07603618,  0.03904646, ...,  0.03761718,\n",
       "          -0.01233643,  0.06865104]],\n",
       " \n",
       "        [[-0.07431366, -0.11635654,  0.03024611, ...,  0.0690169 ,\n",
       "           0.03337203,  0.13259956]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.03395545, -0.09449603,  0.01328527, ...,  0.05414047,\n",
       "          -0.00072304,  0.08297522]],\n",
       " \n",
       "        [[-0.04677773, -0.06725947,  0.03903989, ...,  0.04469272,\n",
       "           0.0060822 ,  0.05386278]],\n",
       " \n",
       "        [[-0.13497181, -0.11935817,  0.03368724, ...,  0.10156236,\n",
       "           0.09774466,  0.00426541]]], dtype=float32)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_info[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       track_id         artist_name            track_title\n",
      "21754     33986  The Pleasure Kills  Pictures On The Floor\n"
     ]
    }
   ],
   "source": [
    "# d) Recuperar os metadados\n",
    "#data = metadata_df.loc(metadata_df[\"track_id\"]==I)\n",
    "data = metadata_df.loc[metadata_df[\"track_id\"] == int(obras_result)]\n",
    "\n",
    "# e) Retornar ao Cliente\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
