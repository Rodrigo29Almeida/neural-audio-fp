{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import click\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from model.dataset import Dataset\n",
    "from model.fp.melspec.melspectrogram import get_melspec_layer\n",
    "from model.fp.specaug_chain.specaug_chain import get_specaug_chain_layer\n",
    "from model.fp.nnfp import get_fingerprinter\n",
    "from model.fp.NTxent_loss_single_gpu import NTxentLoss\n",
    "from model.fp.online_triplet_loss import OnlineTripletLoss\n",
    "from model.fp.lamb_optimizer import LAMB\n",
    "\n",
    "\n",
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_specaug: spec-augmentation layer.\n",
    "    m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "    assert(m_specaug.bypass==False) # Detachable by setting m_specaug.bypass.\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    return m_pre, m_specaug, m_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_fname):\n",
    "    config_filepath = '../config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ../config/default.yaml\n"
     ]
    }
   ],
   "source": [
    "config:str = \"default\" \n",
    "cfg = load_config(config)\n",
    "dataset = Dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning schedule\n",
    "total_nsteps = cfg['TRAIN']['MAX_EPOCH'] * len(dataset.get_train_ds())\n",
    "if cfg['TRAIN']['LR_SCHEDULE'].upper() == 'COS':\n",
    "    lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "        initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "        decay_steps=total_nsteps,\n",
    "        alpha=1e-06)\n",
    "elif cfg['TRAIN']['LR_SCHEDULE'].upper() == 'COS-RESTART':\n",
    "    lr_schedule = tf.keras.experimental.CosineDecayRestarts(\n",
    "        initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "        first_decay_steps=int(total_nsteps * 0.1),\n",
    "        num_periods=0.5,\n",
    "        alpha=2e-06)\n",
    "else:\n",
    "    lr_schedule = float(cfg['TRAIN']['LR'])\n",
    "\n",
    "# Optimizer\n",
    "if cfg['TRAIN']['OPTIMIZER'].upper() == 'LAMB':\n",
    "    opt = LAMB(learning_rate=lr_schedule)\n",
    "elif cfg['TRAIN']['OPTIMIZER'].upper() == 'ADAM':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "else:\n",
    "    raise NotImplementedError(cfg['TRAIN']['OPTIMIZER'])\n",
    "\n",
    "# Loss objects\n",
    "if cfg['LOSS']['LOSS_MODE'].upper() == 'NTXENT': # Default\n",
    "    loss_obj_train = NTxentLoss(\n",
    "        n_org=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "        n_rep=cfg['BSZ']['TR_BATCH_SZ'] - cfg['BSZ']['TR_N_ANCHOR'],\n",
    "        tau=cfg['LOSS']['TAU'])\n",
    "    loss_obj_val = NTxentLoss(\n",
    "        n_org=cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "        n_rep=cfg['BSZ']['VAL_BATCH_SZ'] - cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "        tau=cfg['LOSS']['TAU'])\n",
    "elif cfg['LOSS']['LOSS_MODE'].upper() == 'ONLINE-TRIPLET': # Now-playing\n",
    "    loss_obj_train = OnlineTripletLoss(\n",
    "        bsz=cfg['BSZ']['TR_BATCH_SZ'],\n",
    "        n_anchor=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "        mode = 'semi-hard',\n",
    "        margin=cfg['LOSS']['MARGIN'])\n",
    "    loss_obj_val = OnlineTripletLoss(\n",
    "        bsz=cfg['BSZ']['VAL_BATCH_SZ'],\n",
    "        n_anchor=cfg['BSZ']['VAL_N_ANCHOR'],\n",
    "        mode = 'all', # use 'all' mode for validation\n",
    "        margin=0.)\n",
    "else:\n",
    "    raise NotImplementedError(cfg['LOSS']['LOSS_MODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.optimizers.adam.Adam at 0x7f4a5cbd8d50>,\n",
       " <model.fp.NTxent_loss_single_gpu.NTxentLoss at 0x7f4aac64d7d0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt, loss_obj_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre, m_specaug, m_fp = build_fp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restaurado de /home/rodrigo/Documents/neural-audio-fp/logs/checkpoint/CHECKPOINT_BSZ_120/ckpt-100\n"
     ]
    }
   ],
   "source": [
    "#m_fp.load_weights('/home/rodrigo/Documents/neural-audio-fp/logs/checkpoint/CHECKPOINT_BSZ_120/')\n",
    "# Setup do checkpoint manager\n",
    "checkpoint_path = '/home/rodrigo/Documents/neural-audio-fp/logs/checkpoint/CHECKPOINT_BSZ_120/'\n",
    "#optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=m_fp)\n",
    "checkpoint_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, \n",
    "    checkpoint_path, \n",
    "    max_to_keep=3, \n",
    "    keep_checkpoint_every_n_hours=1\n",
    ")\n",
    "\n",
    "# Carregar o Ãºltimo checkpoint\n",
    "checkpoint.restore(checkpoint_manager.latest_checkpoint).expect_partial()\n",
    "\n",
    "if checkpoint_manager.latest_checkpoint:\n",
    "    print(f'Checkpoint restaurado de {checkpoint_manager.latest_checkpoint}')\n",
    "else:\n",
    "    print('Nenhum checkpoint encontrado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "def convlayer(hidden_ch=128,\n",
    "              strides=[(1,1),(1,1)],\n",
    "              norm='layer_norm2d'):\n",
    "    conv2d_1x3 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                        kernel_size=(1, 3),\n",
    "                                        strides=strides[0],\n",
    "                                        padding='SAME',\n",
    "                                        dilation_rate=(1, 1),\n",
    "                                        kernel_initializer='glorot_uniform',\n",
    "                                        bias_initializer='zeros')\n",
    "    conv2d_3x1 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                        kernel_size=(3, 1),\n",
    "                                        strides=strides[1],\n",
    "                                        padding='SAME',\n",
    "                                        dilation_rate=(1, 1),\n",
    "                                        kernel_initializer='glorot_uniform',\n",
    "                                        bias_initializer='zeros')\n",
    "    if norm == 'layer_norm1d':\n",
    "        BN_1x3 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        BN_3x1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "    elif norm == 'layer_norm2d':\n",
    "        BN_1x3 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "        BN_3x1 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "    else:\n",
    "        BN_1x3 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        BN_3x1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        \n",
    "    forward = tf.keras.Sequential([conv2d_1x3,\n",
    "                                   tf.keras.layers.ELU(),\n",
    "                                   BN_1x3,\n",
    "                                   conv2d_3x1,\n",
    "                                   tf.keras.layers.ELU(),\n",
    "                                   BN_3x1\n",
    "                                   ])\n",
    "    \n",
    "    return forward\n",
    "\n",
    "\n",
    "\n",
    "def create_sequential_front_conv(input_shape=(256,32,1),\n",
    "                                 emb_sz=128,\n",
    "                                 front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024],\n",
    "                                 front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                                [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                                [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "                                                [(1,1), (2,1)], [(1,2), (2,1)]],\n",
    "                                 norm='layer_norm2d'):\n",
    "    front_conv = tf.keras.Sequential(name='ConvLayers')\n",
    "    if ((front_hidden_ch[-1] % emb_sz) != 0):\n",
    "        front_hidden_ch[-1] = ((front_hidden_ch[-1]//emb_sz) + 1) * emb_sz\n",
    "\n",
    "    for i in range(len(front_strides)):\n",
    "        front_conv.add(convlayer(hidden_ch=front_hidden_ch[i], strides=front_strides[i], norm=norm))\n",
    "    front_conv.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    return front_conv\n",
    "\n",
    "\n",
    "\n",
    "def auxiliar(input1):\n",
    "    conv_layer = create_sequential_front_conv(input_shape=(256,32,1),\n",
    "                                               emb_sz=128,\n",
    "                                               front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024],\n",
    "                                               front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                                              [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                                              [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "                                                              [(1,1), (2,1)], [(1,2), (2,1)]],\n",
    "                                               norm='layer_norm2d')\n",
    "\n",
    "    unit_dim = [32, 1]\n",
    "    q = 128\n",
    "    arquiteturas_densas = tf.keras.Sequential([tf.keras.layers.Dense(unit_dim[0], activation='elu'),\n",
    "                                               tf.keras.layers.Dense(unit_dim[1])])\n",
    "\n",
    "    x = input1\n",
    "    #x reshape\n",
    "    x = conv_layer(x)\n",
    "\n",
    "    y_list = [0] * q\n",
    "    x_split = tf.split(x, num_or_size_splits=128, axis=1)\n",
    "\n",
    "    for v, k in enumerate(x_split):\n",
    "        y_list[v] = arquiteturas_densas(k)\n",
    "\n",
    "    out = tf.concat(y_list, axis=1)\n",
    "    output = tf.math.l2_normalize(out, axis=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_fingerprinting(input1):\n",
    "    output = auxiliar(input1)\n",
    "    fingerprinting_model = Model(inputs=input1, outputs=output)\n",
    "    return fingerprinting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fp.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desenho do sistema, com os blocos de projeto.\n",
    "\n",
    "o kapre num fluxo completo (modelo antigo)\n",
    "\n",
    "dividir o projeto em blocos e depois substituir o que tenho\n",
    "depois dentro dos blocos, os subblocos, gradio etc\n",
    "\n",
    "subclassing -> kapre ->\n",
    "functional -> triangular ->\n",
    "\n",
    "predict, extraÃ§ao, faiss\n",
    "\n",
    "base de dados, cap. resultados, o que preciso\n",
    "introduÃ§Ã£o, ver as comparaÃ§Ãµes todas\n",
    "desenho e explicaÃ§Ã£o dos blocos do projeto\n",
    "descriÃ§Ã£o do functional e do subclassing com o keras - breve revisÃ£o do keras.tensofrlow, existe tal e tal e vantagens e desvantagens, como Ã© bom para o projeto, porque objetivo\n",
    "sumÃ¡rio da tese - todos os titulos do capÃ­tulo (sections, subsections etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transferir os pesos do modelo subclassed para o modelo funcional\n",
    "\n",
    "functional_model=get_fingerprinting(input1)\n",
    "functional_model.set_weights(m_fp.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
