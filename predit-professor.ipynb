{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import wave\n",
    "\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "from model.fp.melspec.melspectrogram import get_melspec_layer\n",
    "from model.fp.specaug_chain.specaug_chain import get_specaug_chain_layer\n",
    "from model.fp.nnfp import get_fingerprinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_specaug: spec-augmentation layer.\n",
    "    #m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "    #assert(m_specaug.bypass==False) # Detachable by setting m_specaug.bypass.\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    m_fp.trainable = False\n",
    "    \n",
    "    return m_pre, m_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_fname):\n",
    "    config_filepath = './config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def predict(X, m_fp):\n",
    "    \"\"\" \n",
    "    Test step used for mini-search-validation \n",
    "    X -> (B,1,8000)\n",
    "    \"\"\"\n",
    "    emb_gf = m_fp(X)\n",
    "\n",
    "    return emb_gf\n",
    "\n",
    "\n",
    "def load_model():\n",
    "\n",
    "    checkpoint_name_dir:str = \"./logs/CHECKPOINT_BSZ_120\"  #\"CHECKPOINT\"   # string\n",
    "    config:str = \"default\"   \n",
    "\n",
    "    cfg = load_config(config)\n",
    "\n",
    "    m_pre, m_fp = build_fp(cfg)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(m_fp)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_name_dir))\n",
    "        \n",
    "    return m_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def predict(X, m_fp):\n",
    "    \"\"\" \n",
    "    Test step used for mini-search-validation \n",
    "    X -> (B,1,8000)\n",
    "    \"\"\"\n",
    "    emb_gf = m_fp(X)\n",
    "\n",
    "    return emb_gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:/mnt/dataset/public/Fingerprinting/teste/uniqueFile/002000.wav\n",
      "seg_idx:3\n",
      "offset_min:-4000\n",
      "offset_max:4000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audioDir = '/mnt/dataset/public/Fingerprinting/teste/uniqueFile/002000.wav'\n",
    "\n",
    "fns_list = sorted(glob.glob(audioDir, recursive=True))\n",
    "\n",
    "fns_seg_list = []\n",
    "seg_list_test = []\n",
    "\n",
    "fs:int = 8000\n",
    "duration:int = 1\n",
    "hop:int = 0.5\n",
    "\n",
    "for offset_idx, filename in enumerate(fns_list):\n",
    "    # Get audio info\n",
    "    n_frames_in_seg = fs * duration\n",
    "    n_frames_in_hop = fs * hop  # 2019 09.05\n",
    "    file_ext = filename[-3:]\n",
    "\n",
    "    if file_ext == 'wav':\n",
    "        pt_wav = wave.open(filename, 'r')\n",
    "        _fs = pt_wav.getframerate()\n",
    "\n",
    "        if fs != _fs:\n",
    "            raise ValueError('Sample rate should be {} but got {}'.format(\n",
    "                str(fs), str(_fs)))\n",
    "\n",
    "        n_frames = pt_wav.getnframes()\n",
    "        #n_segs = n_frames // n_frames_in_seg\n",
    "        if n_frames > n_frames_in_seg:\n",
    "            n_segs = (n_frames - n_frames_in_seg +\n",
    "                        n_frames_in_hop) // n_frames_in_hop\n",
    "        else:\n",
    "            n_segs = 1\n",
    "\n",
    "        n_segs = int(n_segs)\n",
    "        assert (n_segs > 0)\n",
    "        residual_frames = np.max([\n",
    "            0,\n",
    "            n_frames - ((n_segs - 1) * n_frames_in_hop + n_frames_in_seg)\n",
    "        ])\n",
    "        \n",
    "        #print(f\"filename:{filename}\\nn_frames_in_seg:{n_frames_in_seg}\\nn_frames_in_hop:{n_frames_in_hop}\\nfile_ext:{file_ext}\\n\")\n",
    "        #print(f\"_fs:{_fs}\\nn_frames:{n_frames}\\nn_segs:{n_segs}\\nresidual_frames:{residual_frames}\\n\")\n",
    "        \n",
    "        pt_wav.close()\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(file_ext)\n",
    "    \n",
    "\n",
    "    for seg_idx in range(n_segs):\n",
    "        offset_min, offset_max = int(-1 * n_frames_in_hop), n_frames_in_hop\n",
    "        \n",
    "        #print(f\"seg_idx:{seg_idx}\\noffset_min:{offset_min}\\noffset_max:{offset_max}\\nfns_seg_list:{fns_seg_list}\\n\")\n",
    "\n",
    "        if seg_idx == 0:  # first seg\n",
    "            offset_min = 0\n",
    "        if seg_idx == (n_segs - 1):  # last seg\n",
    "            offset_max = residual_frames\n",
    "        if seg_idx == 3:\n",
    "            seg_list_test.append( [filename, seg_idx, offset_min, offset_max])\n",
    "            print(f\"filename:{filename}\\nseg_idx:{seg_idx}\\noffset_min:{offset_min}\\noffset_max:{offset_max}\\n\")\n",
    "\n",
    "        fns_seg_list.append(\n",
    "            [filename, seg_idx, offset_min, offset_max])\n",
    "        \n",
    "        #print(f\"seg_idx:{seg_idx}\\noffset_min:{offset_min}\\noffset_max:{offset_max}\\nfns_seg_list:{fns_seg_list}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(filename=str(),\n",
    "               seg_start_sec=float(),\n",
    "               offset_sec=0.0,\n",
    "               seg_length_sec=float(),\n",
    "               seg_pad_offset_sec=0.0,\n",
    "               fs=22050,\n",
    "               amp_mode='normal'):\n",
    "    \"\"\"\n",
    "        Open file to get file info --> Calulate index range\n",
    "        --> Load sample by index --> Padding --> Max-Normalize --> Out\n",
    "        \n",
    "    \"\"\"\n",
    "    start_frame_idx = np.floor((seg_start_sec + offset_sec) * fs).astype(int)\n",
    "    seg_length_frame = np.floor(seg_length_sec * fs).astype(int)\n",
    "    end_frame_idx = start_frame_idx + seg_length_frame\n",
    "\n",
    "    # Get file-info\n",
    "    file_ext = filename[-3:]\n",
    "    print(start_frame_idx, end_frame_idx)\n",
    "\n",
    "    if file_ext == 'wav':\n",
    "        pt_wav = wave.open(filename, 'r')\n",
    "        pt_wav.setpos(start_frame_idx)\n",
    "        x = pt_wav.readframes(end_frame_idx - start_frame_idx)\n",
    "        x = np.frombuffer(x, dtype=np.int16)\n",
    "        x = x / 2**15  # dtype=float\n",
    "    else:\n",
    "        raise NotImplementedError(file_ext)\n",
    "\n",
    "    # Max Normalize, random amplitude\n",
    "    if amp_mode == 'normal':\n",
    "        pass\n",
    "    elif amp_mode == 'max_normalize':\n",
    "        _x_max = np.max(np.abs(x))\n",
    "        if _x_max != 0:\n",
    "            x = x / _x_max\n",
    "    else:\n",
    "        raise ValueError('amp_mode={}'.format(amp_mode))\n",
    "\n",
    "    # padding process. it works only when win_size> audio_size and padding='random'\n",
    "    audio_arr = np.zeros(int(seg_length_sec * fs))\n",
    "    seg_pad_offset_idx = int(seg_pad_offset_sec * fs)\n",
    "    audio_arr[seg_pad_offset_idx:seg_pad_offset_idx + len(x)] = x\n",
    "    return audio_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 20000\n"
     ]
    }
   ],
   "source": [
    "x = load_audio(filename=filename,\n",
    "               seg_start_sec=1.5,\n",
    "               seg_length_sec=1,\n",
    "               fs=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(8000,)\n",
      "[0.0213623  0.01916504 0.021698   ... 0.02557373 0.0295105  0.02172852]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(type(x))\n",
    "print(np.shape(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 8000)\n",
      "[[0.0213623  0.01916504 0.021698   ... 0.02557373 0.0295105  0.02172852]]\n"
     ]
    }
   ],
   "source": [
    "print(type(x))\n",
    "print(np.shape(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x, m_fp):\n",
    "    '''\n",
    "    filepath: (str, ndarray)\n",
    "    '''\n",
    "\n",
    "    # tenho as tramas e de 8000 amostras tenho de chamar a get_melspec, tenho de trazer o get_melspec e o get_fringerprint, tenho trazer uma matriz do input_shape para que consigo ver os pesos\n",
    "    # tem de ser m_spec, _, m_fp = build_fp(cfg). matching. tirar o librosa pegar no sinal, passar no fingerprint. 3 tensores, um para mel spec, um para data_aug e outro para a rede.\n",
    "    # (256,32,1) de entrada no X, tenho de alterar em vez de 8000\n",
    "    \n",
    "    emb = predict(x, m_fp)\n",
    "\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "\n",
    "    return emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ./config/default.yaml\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_3084456/2396453651.py\", line 7, in predict  *\n        emb_gf = m_fp(X)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file7mrpzq9p.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).front_conv, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_fileu8kwn9i4.py\", line 15, in tf__call\n        raise\n\n    ValueError: Exception encountered when calling layer 'finger_printer_5' (type FingerPrinter).\n    \n    in user code:\n    \n        File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 229, in call  *\n            x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n        File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileu8kwn9i4.py\", line 15, in tf__call\n            raise\n    \n        ValueError: Exception encountered when calling layer 'conv_layer_40' (type ConvLayer).\n        \n        in user code:\n        \n            File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 83, in call  *\n                return self.forward(x)\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n                raise ValueError(\n        \n            ValueError: Exception encountered when calling layer 'sequential_683' (type Sequential).\n            \n            Input 0 of layer \"conv2d_80\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (1, 8000)\n            \n            Call arguments received by layer 'sequential_683' (type Sequential):\n              • inputs=tf.Tensor(shape=(1, 8000), dtype=float32)\n              • training=None\n              • mask=None\n        \n        \n        Call arguments received by layer 'conv_layer_40' (type ConvLayer):\n          • x=tf.Tensor(shape=(1, 8000), dtype=float32)\n    \n    \n    Call arguments received by layer 'finger_printer_5' (type FingerPrinter):\n      • inputs=tf.Tensor(shape=(1, 8000), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m_fp \u001b[38;5;241m=\u001b[39m load_model()\n\u001b[0;32m----> 2\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_fp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[114], line 10\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(x, m_fp)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mfilepath: (str, ndarray)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tenho as tramas e de 8000 amostras tenho de chamar a get_melspec, tenho de trazer o get_melspec e o get_fringerprint, tenho trazer uma matriz do input_shape para que consigo ver os pesos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# tem de ser m_spec, _, m_fp = build_fp(cfg). matching. tirar o libros,a pegar no sinla, passar no fingerprint. 3 tensores, um para mel spec, um para data_aug e outro para a rede.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# (256,32,1) de entrada no X, tenho de alterar em vez de 8000\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_fp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_functions_eagerly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m emb\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekdc7cggg.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict\u001b[0;34m(X, m_fp)\u001b[0m\n\u001b[1;32m     12\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 14\u001b[0m emb_gf \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_fp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file7mrpzq9p.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfront_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv_enc, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileu8kwn9i4.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mforward, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_3084456/2396453651.py\", line 7, in predict  *\n        emb_gf = m_fp(X)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file7mrpzq9p.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).front_conv, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_fileu8kwn9i4.py\", line 15, in tf__call\n        raise\n\n    ValueError: Exception encountered when calling layer 'finger_printer_5' (type FingerPrinter).\n    \n    in user code:\n    \n        File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 229, in call  *\n            x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n        File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileu8kwn9i4.py\", line 15, in tf__call\n            raise\n    \n        ValueError: Exception encountered when calling layer 'conv_layer_40' (type ConvLayer).\n        \n        in user code:\n        \n            File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 83, in call  *\n                return self.forward(x)\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n                raise ValueError(\n        \n            ValueError: Exception encountered when calling layer 'sequential_683' (type Sequential).\n            \n            Input 0 of layer \"conv2d_80\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (1, 8000)\n            \n            Call arguments received by layer 'sequential_683' (type Sequential):\n              • inputs=tf.Tensor(shape=(1, 8000), dtype=float32)\n              • training=None\n              • mask=None\n        \n        \n        Call arguments received by layer 'conv_layer_40' (type ConvLayer):\n          • x=tf.Tensor(shape=(1, 8000), dtype=float32)\n    \n    \n    Call arguments received by layer 'finger_printer_5' (type FingerPrinter):\n      • inputs=tf.Tensor(shape=(1, 8000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m_fp = load_model()\n",
    "emb = run(x, m_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seg_list_test[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nseg_list_test[0][0]\\nseg_list_test[0][2]\\nduration = 1\\nfs = 8000\\namp_mode = 'normal'\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "seg_list_test[0][0]\n",
    "seg_list_test[0][2]\n",
    "duration = 1\n",
    "fs = 8000\n",
    "amp_mode = 'normal'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xs = load_audio_multi_start(fns_event_seg_list[idx][0], start_sec_list, duration, fs, amp_mode)  # xs: ((1+n_pos)),T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 'layer_norm2d'\n",
    "q = 128\n",
    "\n",
    "if norm in ['layer_norm1d', 'layer_norm2d']:\n",
    "    BN = [tf.keras.layers.LayerNormalization(axis=-1) for i in range(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e0510>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16d66210>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16d50450>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab15c1d550>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162f06d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab15c55590>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16bfda50>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1620c290>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e8c10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162f0150>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162c59d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1624efd0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab176c5310>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab166a7290>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162413d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162afad0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab15d36a10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab160412d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16a5c790>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16043fd0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162c3290>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e12d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1623af90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16000650>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162eb610>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162205d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16001c90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e01d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162f4e50>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16d52110>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162ac750>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162b6850>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162da690>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162c15d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162d9390>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603f590>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16001650>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16e6f3d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162f1510>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603de50>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603e590>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603dd10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16015550>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16059390>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162db350>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16001410>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab15dbaf10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1605a010>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1605a810>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16000910>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162f04d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162d9d50>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16003390>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16d51490>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16678b90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16062710>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162eb510>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16063990>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab17523790>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162db910>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162db8d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16060610>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16236390>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16048210>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1604ad10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab15c3a3d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1624c610>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16a77690>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16015790>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162eaa90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16d9e350>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16049c10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16283790>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1604ba90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162f0790>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16241290>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab151a57d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1607a5d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1607b490>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e3850>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16000410>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16bfe490>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16274590>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16d2f210>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16003490>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16000450>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16082310>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16040850>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16a5bfd0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab160287d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab15c54550>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16059e90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16238b10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162ea310>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608a2d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16088850>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab160882d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e8a90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16bfe290>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608c4d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16003a10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16622b90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608e350>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608e250>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608cd90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab160013d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16236450>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1624c4d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608f5d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16e94550>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab160285d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16bff250>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603ca90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16da30d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608da90>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab16291790>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603b4d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162eac10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603b650>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1603b850>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162f7650>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e37d0>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1608c450>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e0490>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162e1e10>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab1620e610>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162cb410>,\n",
       " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7aab162b7210>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RASCUNHOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = predict(X, m_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ./config/default.yaml\n"
     ]
    }
   ],
   "source": [
    "modelo = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.fp.nnfp.FingerPrinter"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model.fp.nnfp.DivEncLayer at 0x7aab17659750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.div_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"default\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ./config/default.yaml\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/dataset/public/Fingerprinting/neural-audio-fp-dataset/music/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['DIR']['SOURCE_ROOT_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([5, 1, 2, 4])\n",
    "y=tf.reduce_max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 1 2 4], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 4], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda, Permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=Permute((3, 2, 1), input_shape=x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.layers.reshaping.permute.Permute object at 0x7aab1769bd90>\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_strides=[\n",
    "                [(1,2), (2,1)], \n",
    "                [(1,2), (2,1)],\n",
    "                [(1,2), (2,1)],\n",
    "                [(1,2), (2,1)],\n",
    "                [(1,1), (2,1)],\n",
    "                [(1,2), (2,1)],\n",
    "                [(1,1), (2,1)],\n",
    "                [(1,2), (2,1)]\n",
    "                ]\n",
    "len(front_strides[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2, 2)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(front_strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\"\"\" nnfp.py\n",
    "\n",
    "'Neural Audio Fingerprint for High-specific Audio Retrieval based on \n",
    "Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "\n",
    "USAGE:\n",
    "    \n",
    "    Please see test() in the below.\n",
    "    \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "def ConvLayer(input_layer,\n",
    "              hidden_ch,\n",
    "                 strides,\n",
    "                 norm):\n",
    "    \"\"\"\n",
    "    Separable convolution layer\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hidden_ch: (int)\n",
    "    strides: [(int, int), (int, int)]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. (default)\n",
    "          'layer_norm2d' for normalization on on FxT space \n",
    "          'batch_norm' or else, batch-normalization\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "    [Conv1x3]>>[ELU]>>[BN]>>[Conv3x1]>>[ELU]>>[BN]\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    x: (B,F,T,C) with {F=F/stride, T=T/stride, C=hidden_ch}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #input_layer = #tf.keras.Input(shape=(None, None, 1))\n",
    "    \n",
    "    #Convolution 1x3\n",
    "    conv2d_1x3 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(1, 3),\n",
    "                                                 strides=strides[0],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')(input_layer)\n",
    "\n",
    "    layerNorm_1x3 = tf.keras.layers.ELU()(conv2d_1x3)\n",
    "\n",
    "    if norm == 'layer_norm1d':\n",
    "        BN_1x3 = tf.keras.layers.LayerNormalization(axis=-1)(layerNorm_1x3)\n",
    "    elif norm == 'layer_norm2d':\n",
    "        BN_1x3 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))(layerNorm_1x3)\n",
    "    else:\n",
    "        BN_1x3 = tf.keras.layers.BatchNormalization(axis=-1)(layerNorm_1x3) # Fix axis: 2020 Apr20\n",
    "\n",
    "\n",
    "    #Convolution 3x1\n",
    "    conv2d_3x1 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(3, 1),\n",
    "                                                 strides=strides[1],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')(BN_1x3)\n",
    "\n",
    "    layerNorm_3x1 = tf.keras.layers.ELU()(conv2d_3x1)\n",
    "\n",
    "    if norm == 'layer_norm1d':\n",
    "        BN_3x1 = tf.keras.layers.LayerNormalization(axis=-1)(layerNorm_3x1)\n",
    "    elif norm == 'layer_norm2d':\n",
    "        BN_3x1 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))(layerNorm_3x1)\n",
    "    else:\n",
    "        BN_3x1 = tf.keras.layers.BatchNormalization(axis=-1)(layerNorm_3x1)\n",
    "\n",
    "    \n",
    "    return tf.keras.Model(inputs=input_layer, outputs=BN_3x1)\n",
    "\n",
    "\n",
    "\n",
    "def DivEncLayer(input_layer, q, unit_dim):\n",
    "    \"\"\"\n",
    "    Multi-head projection a.k.a. 'divide and encode' layer:\n",
    "        \n",
    "    • The concept of 'divide and encode' was discovered  in Lai et.al.,\n",
    "     'Simultaneous Feature Learning and Hash Coding with Deep Neural Networks',\n",
    "      2015. https://arxiv.org/abs/1504.03410\n",
    "    • It was also adopted in Gfeller et.al. 'Now Playing: Continuo-\n",
    "      us low-power music recognition', 2017. https://arxiv.org/abs/1711.10958\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    q: (int) number of slices as 'slice_length = input_dim / q'\n",
    "    unit_dim: [(int), (int)]\n",
    "    norm: 'layer_norm1d' or 'layer_norm2d' uses 1D-layer normalization on the feature.\n",
    "          'batch_norm' or else uses batch normalization. Default is 'layer_norm2d'.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    x: (B,1,1,C)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"input_layer-div:{input_layer}\")\n",
    "\n",
    "\n",
    "    #input_layer = tf.keras.Input(shape=(None, None, 1))\n",
    "    flatten_layer = tf.keras.layers.Flatten()(input_layer)\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    for i in range(q):\n",
    "        dense_1 = tf.keras.layers.Dense(unit_dim[0], activation='elu')(flatten_layer)\n",
    "        dense_2 = tf.keras.layers.Dense(unit_dim[1])(dense_1)\n",
    "\n",
    "        \"\"\"if norm in ['layer_norm1d', 'layer_norm2d']:\n",
    "            BN = tf.keras.layers.LayerNormalization(axis=-1)(dense_2)\n",
    "        else:\n",
    "            BN = tf.keras.layers.BatchNormalization(axis=-1)(dense_2)\"\"\"\n",
    "\n",
    "        layers.append(dense_2)\n",
    "\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=tf.keras.layers.Concatenate(axis=1)(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FingerPrinter(input_shape,\n",
    "                 front_hidden_ch,\n",
    "                 front_strides,\n",
    "                 emb_sz,\n",
    "                 fc_unit_dim,\n",
    "                 norm,\n",
    "                 use_L2layer):\n",
    "    \"\"\"\n",
    "    Fingerprinter: 'Neural Audio Fingerprint for High-specific Audio Retrieval\n",
    "        based on Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "    \n",
    "    IN >> [Convlayer]x8 >> [DivEncLayer] >> [L2Normalizer] >> OUT \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_shape: tuple (int), not including the batch size\n",
    "    front_hidden_ch: (list)\n",
    "    front_strides: (list)\n",
    "    emb_sz: (int) default=128\n",
    "    fc_unit_dim: (list) default=[32,1]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. \n",
    "          'layer_norm2d' for normalization on on FxT space (default).\n",
    "          'batch_norm' or else, batch-normalization.\n",
    "    use_L2layer: True (default)\n",
    "    \n",
    "    • Note: batch-normalization will not work properly with TPUs.\n",
    "                    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q) \n",
    "    \n",
    "    \"\"\"\n",
    "    print(input_shape)\n",
    "    input_layer = tf.keras.Input(shape=input_shape, dtype=tf.float32)\n",
    "    #input_layer=input_shape\n",
    "    print(input_layer)\n",
    "\n",
    "\n",
    "    # Front conv layers\n",
    "    for i in range(len(front_strides)):\n",
    "        front_conv = ConvLayer(input_layer=input_layer,hidden_ch=front_hidden_ch[i],\n",
    "            strides=front_strides[i], norm=norm)\n",
    "        \n",
    "        conv = front_conv(input_layer)\n",
    "    \n",
    "    print(f\"conv.shape: {conv.shape}\")\n",
    "    \n",
    "\n",
    "    #podia fazer o flatten aqui?\n",
    "\n",
    "\n",
    "    # Divide & Encoder layer\n",
    "    div_enc_layer = DivEncLayer(input_layer=input_layer, q=emb_sz, unit_dim=fc_unit_dim)\n",
    "    print(f\"div_enc_layer:{div_enc_layer}\")\n",
    "    x = tf.reshape(conv, shape=[conv.shape[0], emb_sz, conv.shape[-1]]) #x = tf.reshape(conv, shape=[conv.shape[0], q, -1])\n",
    "    print(f\"x.shape: {x.shape}\")\n",
    "    x = div_enc_layer(x)\n",
    "\n",
    "    \n",
    "\n",
    "    if use_L2layer:\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(x)\n",
    "        #return tf.math.l2_normalize(x, axis=1)\n",
    "    \n",
    "    return tf.keras.Model(inputs=input_layer, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprinter(cfg, trainable=False):\n",
    "    \"\"\"\n",
    "    Input length : 1s or 2s\n",
    "    \n",
    "    Arguements\n",
    "    ----------\n",
    "    cfg : (dict)\n",
    "        created from the '.yaml' located in /config dicrectory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    <tf.keras.Model> FingerPrinter object\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape = (256, 32, 1)\n",
    "    emb_sz = cfg['MODEL']['EMB_SZ']\n",
    "    norm = cfg['MODEL']['BN']\n",
    "    fc_unit_dim = [32, 1]\n",
    "\n",
    "    front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024]\n",
    "    front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "                [(1,1), (2,1)], [(1,2), (2,1)]]\n",
    "    \n",
    "    model = FingerPrinter(input_shape=input_shape,\n",
    "                          front_hidden_ch=front_hidden_ch,\n",
    "                          front_strides=front_strides,\n",
    "                          emb_sz=emb_sz,\n",
    "                          fc_unit_dim=fc_unit_dim,\n",
    "                          norm=norm)\n",
    "    \n",
    "    model.trainable = trainable\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 32, 1)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256, 32, 1), dtype=tf.float32, name='input_40'), name='input_40', description=\"created by layer 'input_40'\")\n",
      "conv.shape: (None, 128, 16, 1024)\n",
      "input_layer-div:KerasTensor(type_spec=TensorSpec(shape=(None, 256, 32, 1), dtype=tf.float32, name='input_40'), name='input_40', description=\"created by layer 'input_40'\")\n",
      "div_enc_layer:<keras.src.engine.functional.Functional object at 0x7aab08139dd0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"tf.reshape_5\" (type TFOpLambda).\n\nFailed to convert elements of [None, 128, 1024] to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n\nCall arguments received by layer \"tf.reshape_5\" (type TFOpLambda):\n  • tensor=tf.Tensor(shape=(None, 128, 16, 1024), dtype=float32)\n  • shape=['None', '128', '1024']\n  • name=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m front_strides\u001b[38;5;241m=\u001b[39m[[(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)], [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)],\n\u001b[1;32m      6\u001b[0m             [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)], [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)],\n\u001b[1;32m      7\u001b[0m             [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)], [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)],\n\u001b[1;32m      8\u001b[0m             [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)], [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)]]\n\u001b[1;32m     10\u001b[0m use_L2layer:\u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m fprinter \u001b[38;5;241m=\u001b[39m \u001b[43mFingerPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_1s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfront_hidden_ch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfront_hidden_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfront_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfront_strides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_sz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc_unit_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer_norm2d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_L2layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_L2layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m emb_1s \u001b[38;5;241m=\u001b[39m fprinter(input_1s) \u001b[38;5;66;03m# BxD\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[243], line 61\u001b[0m, in \u001b[0;36mFingerPrinter\u001b[0;34m(input_shape, front_hidden_ch, front_strides, emb_sz, fc_unit_dim, norm, use_L2layer)\u001b[0m\n\u001b[1;32m     59\u001b[0m div_enc_layer \u001b[38;5;241m=\u001b[39m DivEncLayer(input_layer\u001b[38;5;241m=\u001b[39minput_layer, q\u001b[38;5;241m=\u001b[39memb_sz, unit_dim\u001b[38;5;241m=\u001b[39mfc_unit_dim)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv_enc_layer:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiv_enc_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#x = tf.reshape(conv, shape=[conv.shape[0], q, -1])\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m div_enc_layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/layers/core/tf_op_layer.py:119\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(x, keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten([args, kwargs])\n\u001b[1;32m    118\u001b[0m ):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTFOpLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"tf.reshape_5\" (type TFOpLambda).\n\nFailed to convert elements of [None, 128, 1024] to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n\nCall arguments received by layer \"tf.reshape_5\" (type TFOpLambda):\n  • tensor=tf.Tensor(shape=(None, 128, 16, 1024), dtype=float32)\n  • shape=['None', '128', '1024']\n  • name=None"
     ]
    }
   ],
   "source": [
    "#input_1s = tf.constant(np.random.randn(3,256,32,1), dtype=tf.float32) # BxFxTx1\n",
    "input_1s=(256, 32, 1)\n",
    "\n",
    "front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024]\n",
    "front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "            [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "            [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "            [(1,1), (2,1)], [(1,2), (2,1)]]\n",
    "\n",
    "use_L2layer:bool = True\n",
    "\n",
    "\n",
    "fprinter = FingerPrinter(input_shape=input_1s, front_hidden_ch=front_hidden_ch, front_strides=front_strides, emb_sz=128, fc_unit_dim=[32, 1], norm='layer_norm2d', use_L2layer=use_L2layer)\n",
    "emb_1s = fprinter(input_1s) # BxD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m norm \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODEL\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m fc_unit_dim \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFingerPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_1s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43memb_sz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_sz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfc_unit_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfc_unit_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[170], line 176\u001b[0m, in \u001b[0;36mFingerPrinter\u001b[0;34m(input_shape, front_hidden_ch, front_strides, emb_sz, fc_unit_dim, norm, use_L2layer)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFingerPrinter\u001b[39m(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    135\u001b[0m                  front_hidden_ch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m],\n\u001b[1;32m    136\u001b[0m                  front_strides\u001b[38;5;241m=\u001b[39m[[(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)], [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m                  norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_norm2d\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    143\u001b[0m                  use_L2layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Fingerprinter: 'Neural Audio Fingerprint for High-specific Audio Retrieval\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m        based on Contrastive Learning', https://arxiv.org/abs/2010.11910\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     input_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Front conv layers\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(front_strides)):\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:297\u001b[0m, in \u001b[0;36m_EagerTensorBase.__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__index__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__index__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "input_1s = tf.constant(np.random.randn(3,256,32,1), dtype=tf.float32) # BxFxTx1\n",
    "emb_sz = cfg['MODEL']['EMB_SZ']\n",
    "norm = cfg['MODEL']['BN']\n",
    "fc_unit_dim = [32, 1]\n",
    "\n",
    "model = FingerPrinter(input_shape=input_1s,\n",
    "                    emb_sz=emb_sz,\n",
    "                    fc_unit_dim=fc_unit_dim,\n",
    "                    norm=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rascunho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\"\"\" nnfp.py\n",
    "\n",
    "'Neural Audio Fingerprint for High-specific Audio Retrieval based on \n",
    "Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "\n",
    "USAGE:\n",
    "    \n",
    "    Please see test() in the below.\n",
    "    \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "\n",
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Separable convolution layer\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hidden_ch: (int)\n",
    "    strides: [(int, int), (int, int)]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. (default)\n",
    "          'layer_norm2d' for normalization on on FxT space \n",
    "          'batch_norm' or else, batch-normalization\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "    [Conv1x3]>>[ELU]>>[BN]>>[Conv3x1]>>[ELU]>>[BN]\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    x: (B,F,T,C) with {F=F/stride, T=T/stride, C=hidden_ch}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_ch=128,\n",
    "                 strides=[(1,1),(1,1)],\n",
    "                 norm='layer_norm2d'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv2d_1x3 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(1, 3),\n",
    "                                                 strides=strides[0],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        self.conv2d_3x1 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(3, 1),\n",
    "                                                 strides=strides[1],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        \n",
    "        if norm == 'layer_norm1d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        elif norm == 'layer_norm2d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "        else:\n",
    "            self.BN_1x3 = tf.keras.layers.BatchNormalization(axis=-1) # Fix axis: 2020 Apr20\n",
    "            self.BN_3x1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "            \n",
    "        self.forward = tf.keras.Sequential([self.conv2d_1x3,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_1x3,\n",
    "                                            self.conv2d_3x1,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_3x1\n",
    "                                            ])\n",
    "\n",
    "       \n",
    "    def call(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class DivEncLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-head projection a.k.a. 'divide and encode' layer:\n",
    "        \n",
    "    • The concept of 'divide and encode' was discovered  in Lai et.al.,\n",
    "     'Simultaneous Feature Learning and Hash Coding with Deep Neural Networks',\n",
    "      2015. https://arxiv.org/abs/1504.03410\n",
    "    • It was also adopted in Gfeller et.al. 'Now Playing: Continuo-\n",
    "      us low-power music recognition', 2017. https://arxiv.org/abs/1711.10958\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    q: (int) number of slices as 'slice_length = input_dim / q'\n",
    "    unit_dim: [(int), (int)]\n",
    "    norm: 'layer_norm1d' or 'layer_norm2d' uses 1D-layer normalization on the feature.\n",
    "          'batch_norm' or else uses batch normalization. Default is 'layer_norm2d'.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    x: (B,1,1,C)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, q=128, unit_dim=[32, 1], norm='batch_norm'):\n",
    "        super(DivEncLayer, self).__init__()\n",
    "\n",
    "        self.q = q\n",
    "        self.unit_dim = unit_dim\n",
    "        self.norm = norm\n",
    "        \n",
    "        if norm in ['layer_norm1d', 'layer_norm2d']:\n",
    "            self.BN = [tf.keras.layers.LayerNormalization(axis=-1) for i in range(q)]\n",
    "        else:\n",
    "            self.BN = [tf.keras.layers.BatchNormalization(axis=-1) for i in range(q)]\n",
    "            \n",
    "        self.split_fc_layers = self._construct_layers() \n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Prepare output embedding variable for dynamic batch-size \n",
    "        self.slice_length = int(input_shape[-1] / self.q)\n",
    "\n",
    " \n",
    "    def _construct_layers(self):\n",
    "        layers = list()\n",
    "        for i in range(self.q): # q: num_slices\n",
    "            layers.append(tf.keras.Sequential([tf.keras.layers.Dense(self.unit_dim[0], activation='elu'),\n",
    "                                               #self.BN[i],\n",
    "                                               tf.keras.layers.Dense(self.unit_dim[1])]))\n",
    "        return layers\n",
    "\n",
    " \n",
    "    @tf.function\n",
    "    def _split_encoding(self, x_slices):\n",
    "        \"\"\"\n",
    "        Input: (B,Q,S)\n",
    "        Returns: (B,Q)\n",
    "        \n",
    "        \"\"\"\n",
    "        out = list()\n",
    "        for i in range(self.q):\n",
    "            out.append(self.split_fc_layers[i](x_slices[:, i, :]))\n",
    "        return tf.concat(out, axis=1)\n",
    "\n",
    "    \n",
    "    def call(self, x): # x: (B,1,1,2048)\n",
    "        x = tf.reshape(x, shape=[x.shape[0], self.q, -1]) # (B,Q,S); Q=num_slices; S=slice length; (B,128,8 or 16)\n",
    "        return self._split_encoding(x)\n",
    "    \n",
    "    \n",
    "class FingerPrinter(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Fingerprinter: 'Neural Audio Fingerprint for High-specific Audio Retrieval\n",
    "        based on Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "    \n",
    "    IN >> [Convlayer]x8 >> [DivEncLayer] >> [L2Normalizer] >> OUT \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_shape: tuple (int), not including the batch size\n",
    "    front_hidden_ch: (list)\n",
    "    front_strides: (list)\n",
    "    emb_sz: (int) default=128\n",
    "    fc_unit_dim: (list) default=[32,1]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. \n",
    "          'layer_norm2d' for normalization on on FxT space (default).\n",
    "          'batch_norm' or else, batch-normalization.\n",
    "    use_L2layer: True (default)\n",
    "    \n",
    "    • Note: batch-normalization will not work properly with TPUs.\n",
    "                    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q) \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(256,32,1),\n",
    "                 front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024],\n",
    "                 front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)]],\n",
    "                 emb_sz=128, # q\n",
    "                 fc_unit_dim=[32,1],\n",
    "                 norm='layer_norm2d',\n",
    "                 use_L2layer=True):\n",
    "        super(FingerPrinter, self).__init__()\n",
    "        self.front_hidden_ch = front_hidden_ch\n",
    "        self.front_strides = front_strides\n",
    "        self.emb_sz=emb_sz\n",
    "        self.norm = norm\n",
    "        self.use_L2layer = use_L2layer\n",
    "        \n",
    "        self.n_clayers = len(front_strides)\n",
    "        self.front_conv = tf.keras.Sequential(name='ConvLayers')\n",
    "        if ((front_hidden_ch[-1] % emb_sz) != 0):\n",
    "            front_hidden_ch[-1] = ((front_hidden_ch[-1]//emb_sz) + 1) * emb_sz                \n",
    "        \n",
    "        # Front (sep-)conv layers\n",
    "        #x = tf.zeros((1,)+ input_shape, dtype=tf.float32)\n",
    "        #print(f\"ConvLayer entrada: {self.front_conv(x).shape}\")\n",
    "        for i in range(self.n_clayers):\n",
    "            self.front_conv.add(ConvLayer(hidden_ch=front_hidden_ch[i],\n",
    "                strides=front_strides[i], norm=norm))\n",
    "            #print(f\"ConvLayer {i+1}: {self.front_conv(x).shape}\")\n",
    "\n",
    "        self.front_conv.add(tf.keras.layers.Flatten()) # (B,F',T',C) >> (B,D)\n",
    "\n",
    "        #print(f\"Flatten: {self.front_conv}\") \n",
    "\n",
    "        # Divide & Encoder layer\n",
    "        self.div_enc = DivEncLayer(q=emb_sz, unit_dim=fc_unit_dim, norm=norm)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n",
    "        x = self.div_enc(x) # (B,Q)\n",
    "        if self.use_L2layer:\n",
    "            return tf.math.l2_normalize(x, axis=1) \n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "def get_fingerprinter(cfg, trainable=False):\n",
    "    \"\"\"\n",
    "    Input length : 1s or 2s\n",
    "    \n",
    "    Arguements\n",
    "    ----------\n",
    "    cfg : (dict)\n",
    "        created from the '.yaml' located in /config dicrectory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    <tf.keras.Model> FingerPrinter object\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape = (256, 32, 1) \n",
    "    emb_sz = cfg['MODEL']['EMB_SZ']\n",
    "    norm = cfg['MODEL']['BN']\n",
    "    fc_unit_dim = [32, 1]\n",
    "    \n",
    "    m = FingerPrinter(input_shape=input_shape,\n",
    "                      emb_sz=emb_sz,\n",
    "                      fc_unit_dim=fc_unit_dim,\n",
    "                      norm=norm)\n",
    "    m.trainable = trainable\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLayer entrada: (1, 256, 32, 1)\n",
      "ConvLayer 1: (1, 128, 16, 128)\n",
      "ConvLayer 2: (1, 64, 8, 128)\n",
      "ConvLayer 3: (1, 32, 4, 256)\n",
      "ConvLayer 4: (1, 16, 2, 256)\n",
      "ConvLayer 5: (1, 8, 2, 512)\n",
      "ConvLayer 6: (1, 4, 1, 512)\n",
      "ConvLayer 7: (1, 2, 1, 1024)\n",
      "ConvLayer 8: (1, 1, 1, 1024)\n",
      "Flatten: <keras.src.engine.sequential.Sequential object at 0x7aab074e5910>\n"
     ]
    }
   ],
   "source": [
    "input_1s = tf.constant(np.random.randn(3,256,32,1), dtype=tf.float32) # BxFxTx1\n",
    "fprinter = FingerPrinter(emb_sz=128, fc_unit_dim=[32, 1], norm='layer_norm2d')\n",
    "emb_1s = fprinter(input_1s) # BxD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\"\"\" nnfp.py\n",
    "\n",
    "'Neural Audio Fingerprint for High-specific Audio Retrieval based on \n",
    "Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "\n",
    "USAGE:\n",
    "    \n",
    "    Please see test() in the below.\n",
    "    \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "\n",
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Separable convolution layer\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hidden_ch: (int)\n",
    "    strides: [(int, int), (int, int)]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. (default)\n",
    "          'layer_norm2d' for normalization on on FxT space \n",
    "          'batch_norm' or else, batch-normalization\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "    [Conv1x3]>>[ELU]>>[BN]>>[Conv3x1]>>[ELU]>>[BN]\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    x: (B,F,T,C) with {F=F/stride, T=T/stride, C=hidden_ch}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_ch=128,\n",
    "                 strides=[(1,1),(1,1)],\n",
    "                 norm='layer_norm2d'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv2d_1x3 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(1, 3),\n",
    "                                                 strides=strides[0],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        self.conv2d_3x1 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(3, 1),\n",
    "                                                 strides=strides[1],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        \n",
    "        if norm == 'layer_norm1d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        elif norm == 'layer_norm2d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "        else:\n",
    "            self.BN_1x3 = tf.keras.layers.BatchNormalization(axis=-1) # Fix axis: 2020 Apr20\n",
    "            self.BN_3x1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "            \n",
    "        self.forward = tf.keras.Sequential([self.conv2d_1x3,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_1x3,\n",
    "                                            self.conv2d_3x1,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_3x1\n",
    "                                            ])\n",
    "\n",
    "       \n",
    "    def call(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class DivEncLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-head projection a.k.a. 'divide and encode' layer:\n",
    "        \n",
    "    • The concept of 'divide and encode' was discovered  in Lai et.al.,\n",
    "     'Simultaneous Feature Learning and Hash Coding with Deep Neural Networks',\n",
    "      2015. https://arxiv.org/abs/1504.03410\n",
    "    • It was also adopted in Gfeller et.al. 'Now Playing: Continuo-\n",
    "      us low-power music recognition', 2017. https://arxiv.org/abs/1711.10958\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    q: (int) number of slices as 'slice_length = input_dim / q'\n",
    "    unit_dim: [(int), (int)]\n",
    "    norm: 'layer_norm1d' or 'layer_norm2d' uses 1D-layer normalization on the feature.\n",
    "          'batch_norm' or else uses batch normalization. Default is 'layer_norm2d'.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    x: (B,1,1,C)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, q=128, unit_dim=[32, 1], norm='batch_norm'):\n",
    "        super(DivEncLayer, self).__init__()\n",
    "\n",
    "        self.q = q\n",
    "        self.unit_dim = unit_dim\n",
    "        self.norm = norm\n",
    "        \n",
    "        if norm in ['layer_norm1d', 'layer_norm2d']:\n",
    "            self.BN = [tf.keras.layers.LayerNormalization(axis=-1) for i in range(q)]\n",
    "        else:\n",
    "            self.BN = [tf.keras.layers.BatchNormalization(axis=-1) for i in range(q)]\n",
    "            \n",
    "        self.split_fc_layers = self._construct_layers() \n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Prepare output embedding variable for dynamic batch-size \n",
    "        self.slice_length = int(input_shape[-1] / self.q)\n",
    "\n",
    " \n",
    "    def _construct_layers(self):\n",
    "        layers = list()\n",
    "        for i in range(self.q): # q: num_slices\n",
    "            layers.append(tf.keras.Sequential([tf.keras.layers.Dense(self.unit_dim[0], activation='elu'),\n",
    "                                               #self.BN[i],\n",
    "                                               tf.keras.layers.Dense(self.unit_dim[1])]))\n",
    "        return layers\n",
    "\n",
    " \n",
    "    @tf.function\n",
    "    def _split_encoding(self, x_slices):\n",
    "        \"\"\"\n",
    "        Input: (B,Q,S)\n",
    "        Returns: (B,Q)\n",
    "        \n",
    "        \"\"\"\n",
    "        out = list()\n",
    "        for i in range(self.q):\n",
    "            out.append(self.split_fc_layers[i](x_slices[:, i, :]))\n",
    "        return tf.concat(out, axis=1)\n",
    "\n",
    "    \n",
    "    def call(self, x): # x: (B,1,1,2048)\n",
    "        x = tf.reshape(x, shape=[x.shape[0], self.q, -1]) # (B,Q,S); Q=num_slices; S=slice length; (B,128,8 or 16)\n",
    "        return self._split_encoding(x)\n",
    "    \n",
    "    \n",
    "class FingerPrinter(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Fingerprinter: 'Neural Audio Fingerprint for High-specific Audio Retrieval\n",
    "        based on Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "    \n",
    "    IN >> [Convlayer]x8 >> [DivEncLayer] >> [L2Normalizer] >> OUT \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_shape: tuple (int), not including the batch size\n",
    "    front_hidden_ch: (list)\n",
    "    front_strides: (list)\n",
    "    emb_sz: (int) default=128\n",
    "    fc_unit_dim: (list) default=[32,1]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. \n",
    "          'layer_norm2d' for normalization on on FxT space (default).\n",
    "          'batch_norm' or else, batch-normalization.\n",
    "    use_L2layer: True (default)\n",
    "    \n",
    "    • Note: batch-normalization will not work properly with TPUs.\n",
    "                    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q) \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(256,32,1),\n",
    "                 front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024],\n",
    "                 front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)]],\n",
    "                 emb_sz=128, # q\n",
    "                 fc_unit_dim=[32,1],\n",
    "                 norm='layer_norm2d',\n",
    "                 use_L2layer=True):\n",
    "        super(FingerPrinter, self).__init__()\n",
    "        self.front_hidden_ch = front_hidden_ch\n",
    "        self.front_strides = front_strides\n",
    "        self.emb_sz=emb_sz\n",
    "        self.norm = norm\n",
    "        self.use_L2layer = use_L2layer\n",
    "        \n",
    "        self.n_clayers = len(front_strides)\n",
    "        self.front_conv = tf.keras.Sequential(name='ConvLayers')\n",
    "        if ((front_hidden_ch[-1] % emb_sz) != 0):\n",
    "            front_hidden_ch[-1] = ((front_hidden_ch[-1]//emb_sz) + 1) * emb_sz                \n",
    "        \n",
    "        # Front (sep-)conv layers\n",
    "        #x = tf.zeros((1,)+ input_shape, dtype=tf.float32)\n",
    "        #print(f\"ConvLayer entrada: {self.front_conv(x).shape}\")\n",
    "        for i in range(self.n_clayers):\n",
    "            self.front_conv.add(ConvLayer(hidden_ch=front_hidden_ch[i],\n",
    "                strides=front_strides[i], norm=norm))\n",
    "            #print(f\"ConvLayer {i+1}: {self.front_conv(x).shape}\")\n",
    "        self.front_conv.add(tf.keras.layers.Flatten()) # (B,F',T',C) >> (B,D)\n",
    "            \n",
    "        # Divide & Encoder layer\n",
    "        self.div_enc = DivEncLayer(q=emb_sz, unit_dim=fc_unit_dim, norm=norm)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n",
    "        x = self.div_enc(x) # (B,Q)\n",
    "        if self.use_L2layer:\n",
    "            return tf.math.l2_normalize(x, axis=1) \n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "def get_fingerprinter(cfg, trainable=False):\n",
    "    \"\"\"\n",
    "    Input length : 1s or 2s\n",
    "    \n",
    "    Arguements\n",
    "    ----------\n",
    "    cfg : (dict)\n",
    "        created from the '.yaml' located in /config dicrectory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    <tf.keras.Model> FingerPrinter object\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape = (256, 32, 1) \n",
    "    emb_sz = cfg['MODEL']['EMB_SZ']\n",
    "    norm = cfg['MODEL']['BN']\n",
    "    fc_unit_dim = [32, 1]\n",
    "    \n",
    "    m = FingerPrinter(input_shape=input_shape,\n",
    "                      emb_sz=emb_sz,\n",
    "                      fc_unit_dim=fc_unit_dim,\n",
    "                      norm=norm)\n",
    "    m.trainable = trainable\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1s = tf.constant(np.random.randn(1,256,32,1), dtype=tf.float32) # BxFxTx1\n",
    "fprinter = FingerPrinter(emb_sz=128, fc_unit_dim=[32, 1], norm='layer_norm2d')\n",
    "emb_1s = fprinter(input_1s) # BxD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       "array([[-0.06714662, -0.08877325,  0.18096662,  0.03541807,  0.04442547,\n",
       "         0.11626906, -0.02300515, -0.00791979,  0.06547429, -0.00591726,\n",
       "         0.07874333,  0.03066022,  0.04967431, -0.09657394,  0.01860259,\n",
       "         0.1795162 , -0.09594179, -0.04747898, -0.04731065,  0.0082687 ,\n",
       "         0.06911098,  0.12106568,  0.00540292, -0.01218743, -0.05398494,\n",
       "         0.05139915, -0.10475634,  0.00499371,  0.03684568, -0.09473342,\n",
       "         0.1336947 , -0.02190757, -0.15067072, -0.11182376,  0.04819969,\n",
       "        -0.08911032, -0.23060912,  0.04759725,  0.05027833, -0.02204659,\n",
       "         0.107933  , -0.03450353,  0.10758416, -0.07598781,  0.02761962,\n",
       "        -0.03841873, -0.02068278,  0.01060095, -0.05038963,  0.02484113,\n",
       "        -0.07576414,  0.00573614,  0.02410174,  0.15370564,  0.2229935 ,\n",
       "        -0.00034676, -0.10300653, -0.05699234,  0.04484287, -0.02547638,\n",
       "        -0.0475092 , -0.01074172,  0.19574203,  0.03643182, -0.00032796,\n",
       "        -0.02165386,  0.05053872,  0.07284964,  0.01489076,  0.01903582,\n",
       "         0.06863181,  0.3082773 , -0.05718884, -0.00644855, -0.05442766,\n",
       "         0.00661975,  0.09373621, -0.08742258, -0.04400137,  0.02164165,\n",
       "         0.18008488,  0.00861448, -0.09376489,  0.12762815,  0.02093907,\n",
       "        -0.02628601, -0.0568919 , -0.02856075, -0.03944829, -0.0944749 ,\n",
       "        -0.01903688, -0.0266759 ,  0.01961317, -0.09044758,  0.01862286,\n",
       "         0.06718939,  0.02571321, -0.02351961, -0.04326514, -0.16626176,\n",
       "        -0.00300179,  0.10320621, -0.16112627,  0.08430207,  0.05661881,\n",
       "        -0.04332411, -0.1334616 , -0.04543824, -0.0086612 , -0.1361069 ,\n",
       "         0.04057903,  0.04828296, -0.04317405,  0.09001833, -0.06770308,\n",
       "        -0.07831921,  0.0243969 ,  0.07966858, -0.03128863,  0.03486577,\n",
       "        -0.04039218, -0.10857062,  0.09989372,  0.19636555,  0.07320648,\n",
       "        -0.00845261,  0.04115584, -0.28692082]], dtype=float32)>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fprinter(input_1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       "array([[-0.06714662, -0.08877325,  0.18096662,  0.03541807,  0.04442547,\n",
       "         0.11626906, -0.02300515, -0.00791979,  0.06547429, -0.00591726,\n",
       "         0.07874333,  0.03066022,  0.04967431, -0.09657394,  0.01860259,\n",
       "         0.1795162 , -0.09594179, -0.04747898, -0.04731065,  0.0082687 ,\n",
       "         0.06911098,  0.12106568,  0.00540292, -0.01218743, -0.05398494,\n",
       "         0.05139915, -0.10475634,  0.00499371,  0.03684568, -0.09473342,\n",
       "         0.1336947 , -0.02190757, -0.15067072, -0.11182376,  0.04819969,\n",
       "        -0.08911032, -0.23060912,  0.04759725,  0.05027833, -0.02204659,\n",
       "         0.107933  , -0.03450353,  0.10758416, -0.07598781,  0.02761962,\n",
       "        -0.03841873, -0.02068278,  0.01060095, -0.05038963,  0.02484113,\n",
       "        -0.07576414,  0.00573614,  0.02410174,  0.15370564,  0.2229935 ,\n",
       "        -0.00034676, -0.10300653, -0.05699234,  0.04484287, -0.02547638,\n",
       "        -0.0475092 , -0.01074172,  0.19574203,  0.03643182, -0.00032796,\n",
       "        -0.02165386,  0.05053872,  0.07284964,  0.01489076,  0.01903582,\n",
       "         0.06863181,  0.3082773 , -0.05718884, -0.00644855, -0.05442766,\n",
       "         0.00661975,  0.09373621, -0.08742258, -0.04400137,  0.02164165,\n",
       "         0.18008488,  0.00861448, -0.09376489,  0.12762815,  0.02093907,\n",
       "        -0.02628601, -0.0568919 , -0.02856075, -0.03944829, -0.0944749 ,\n",
       "        -0.01903688, -0.0266759 ,  0.01961317, -0.09044758,  0.01862286,\n",
       "         0.06718939,  0.02571321, -0.02351961, -0.04326514, -0.16626176,\n",
       "        -0.00300179,  0.10320621, -0.16112627,  0.08430207,  0.05661881,\n",
       "        -0.04332411, -0.1334616 , -0.04543824, -0.0086612 , -0.1361069 ,\n",
       "         0.04057903,  0.04828296, -0.04317405,  0.09001833, -0.06770308,\n",
       "        -0.07831921,  0.0243969 ,  0.07966858, -0.03128863,  0.03486577,\n",
       "        -0.04039218, -0.10857062,  0.09989372,  0.19636555,  0.07320648,\n",
       "        -0.00845261,  0.04115584, -0.28692082]], dtype=float32)>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(emb_1s[0], emb_1s[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
