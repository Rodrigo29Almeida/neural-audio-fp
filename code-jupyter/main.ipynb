{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train e validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import click\n",
    "import yaml\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from dataloader_keras import genUnbalSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load da config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_fname):\n",
    "    config_filepath = '../config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ../config/default.yaml\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, mode='train'):\n",
    "        if mode == 'train': \n",
    "            _prefix = 'train-10k-30s/'\n",
    "            self.filepath = cfg['DIR']['SOURCE_ROOT_DIR'] + _prefix + '**/*.wav'\n",
    "        elif mode == 'val':\n",
    "            self.filepath='data/SHS100K-VAL'\n",
    "\n",
    "        # Data location\n",
    "        self.bg_root_dir = cfg['DIR']['BG_ROOT_DIR'] #! é este que nao preciso? ver\n",
    "        self.ir_root_dir = cfg['DIR']['IR_ROOT_DIR']\n",
    "        self.speech_root_dir = cfg['DIR']['SPEECH_ROOT_DIR']\n",
    "\n",
    "        # BSZ\n",
    "        self.tr_batch_sz = cfg['BSZ']['TR_BATCH_SZ']\n",
    "        self.tr_n_anchor = cfg['BSZ']['TR_N_ANCHOR']\n",
    "\n",
    "        # Model parameters\n",
    "        self.dur = cfg['MODEL']['DUR']\n",
    "        self.hop = cfg['MODEL']['HOP']\n",
    "        self.fs = cfg['MODEL']['FS']\n",
    "\n",
    "        # Time-domain augmentation parameter\n",
    "        self.tr_snr = cfg['TD_AUG']['TR_SNR']\n",
    "        self.ts_snr = cfg['TD_AUG']['TS_SNR']\n",
    "        self.val_snr = cfg['TD_AUG']['VAL_SNR']\n",
    "        self.tr_use_bg_aug = cfg['TD_AUG']['TR_BG_AUG']\n",
    "        self.ts_use_bg_aug = cfg['TD_AUG']['TS_BG_AUG']\n",
    "        self.val_use_bg_aug = cfg['TD_AUG']['VAL_BG_AUG']\n",
    "        self.tr_use_ir_aug = cfg['TD_AUG']['TR_IR_AUG']\n",
    "        self.ts_use_ir_aug = cfg['TD_AUG']['TS_IR_AUG']\n",
    "        self.val_use_ir_aug = cfg['TD_AUG']['VAL_IR_AUG']\n",
    "        self.tr_use_speech_aug = cfg['TD_AUG']['TR_SPEECH_AUG']\n",
    "        self.ts_use_speech_aug = cfg['TD_AUG']['TS_SPEECH_AUG']\n",
    "        self.val_use_speech_aug = cfg['TD_AUG']['VAL_SPEECH_AUG']\n",
    "\n",
    "        # Pre-load file paths for augmentation\n",
    "        self.tr_bg_fps = self.ts_bg_fps = self.val_bg_fps = None\n",
    "        self.tr_ir_fps = self.ts_ir_fps = self.val_ir_fps = None\n",
    "        self.tr_speech_fps = self.ts_speech_fps = self.val_speech_fps = None\n",
    "        self.__set_augmentation_fps()\n",
    "\n",
    "        # Source (music) file paths\n",
    "        self.tr_source_fps = self.val_source_fps = None\n",
    "\n",
    "\n",
    "    def train_ds(self):    \n",
    "        self.tr_source_fps = sorted(glob.glob(self.filepath, recursive=True))\n",
    "        \n",
    "        reduce_items_p = cfg['DATA_SEL']['REDUCE_ITEMS_P']\n",
    "\n",
    "        ds = genUnbalSequence(\n",
    "            fns_event_list=self.tr_source_fps,\n",
    "            bsz=self.tr_batch_sz,\n",
    "            n_anchor=self.tr_n_anchor, #ex) bsz=40, n_anchor=8: 4 positive samples per anchor\n",
    "            duration=self.dur,  # duration in seconds\n",
    "            hop=self.hop,\n",
    "            fs=self.fs,\n",
    "            shuffle=True,\n",
    "            random_offset_anchor=True,\n",
    "            bg_mix_parameter=[self.tr_use_bg_aug, self.tr_bg_fps, self.tr_snr],\n",
    "            ir_mix_parameter=[self.tr_use_ir_aug, self.tr_ir_fps],\n",
    "            speech_mix_parameter=[self.tr_use_speech_aug, self.tr_speech_fps,\n",
    "                                  self.tr_snr],\n",
    "            reduce_items_p=reduce_items_p)\n",
    "        return ds\n",
    "    \n",
    "    def __set_augmentation_fps(self):\n",
    "        \"\"\"\n",
    "        Set file path lists:\n",
    "\n",
    "            If validation set was not available, we replace it with subset of\n",
    "            the trainset.\n",
    "\n",
    "        \"\"\"\n",
    "        # File lists for Augmentations\n",
    "        if self.tr_use_bg_aug:\n",
    "            self.tr_bg_fps = sorted(glob.glob(self.bg_root_dir +\n",
    "                                              'tr/**/*.wav', recursive=True))\n",
    "        if self.ts_use_bg_aug:\n",
    "            self.ts_bg_fps = sorted(glob.glob(self.bg_root_dir +\n",
    "                                              'ts/**/*.wav', recursive=True))\n",
    "        if self.val_use_bg_aug:\n",
    "            self.val_bg_fps = sorted(glob.glob(self.bg_root_dir +\n",
    "                                               'tr/**/*.wav', recursive=True))\n",
    "\n",
    "        if self.tr_use_ir_aug:\n",
    "            self.tr_ir_fps = sorted(\n",
    "                glob.glob(self.ir_root_dir + 'tr/**/*.wav', recursive=True))\n",
    "        if self.ts_use_ir_aug:\n",
    "            self.ts_ir_fps = sorted(\n",
    "                glob.glob(self.ir_root_dir + 'ts/**/*.wav', recursive=True))\n",
    "        if self.val_use_ir_aug:\n",
    "            self.val_ir_fps = sorted(\n",
    "                glob.glob(self.ir_root_dir + 'tr/**/*.wav', recursive=True))\n",
    "\n",
    "        if self.tr_use_speech_aug:\n",
    "            self.tr_speech_fps = sorted(\n",
    "                glob.glob(self.speech_root_dir + 'train/**/*.wav',\n",
    "                          recursive=True))\n",
    "        self.ts_speech_fps = sorted(\n",
    "            glob.glob(self.speech_root_dir + 'test/**/*.wav',\n",
    "                      recursive=True))\n",
    "        if self.val_use_speech_aug:\n",
    "            self.val_speech_fps = sorted(\n",
    "                glob.glob(self.speech_root_dir + 'dev/**/*.wav',\n",
    "                          recursive=True))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset('train')\n",
    "train_data = data.train_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próximo passo, meter o dados mais visíveis. Para caso eu queira ver uma musica, seja só fazer um print no main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melspectrogram import get_melspec_layer\n",
    "from specaug_chain import get_specaug_chain_layer\n",
    "from nnfp import get_fingerprinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "# m_specaug: spec-augmentation layer.\n",
    "m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "\n",
    "m_fp = get_fingerprinter(cfg, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class SimpleDense(keras.layers.Layer): # All Keras layers inherit from the base Layer class.\n",
    "\n",
    "  def __init__(self, units, activation=None): #construtor\n",
    "    super().__init__()\n",
    "    self.units = units\n",
    "    self.activation = activation\n",
    "\n",
    "  def build(self, input_shape): # Weight creation takes place in the build() method.\n",
    "    input_dim = input_shape[-1]\n",
    "    self.W = self.add_weight(shape=(input_dim, self.units), initializer=\"random_normal\") # add_weight() is a shortcut method for creating weights. It is also possible to create standalone variables and assign them as layer attributes, like self.W = tf.Variable(tf.random.uniform(w_shape)).\n",
    "    self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\")\n",
    "\n",
    "  def call(self, inputs): # We define the forward pass computation in the call() method.\n",
    "    y = tf.matmul(inputs, self.W) + self.b\n",
    "    if self.activation is not None:\n",
    "      y = self.activation(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Separable convolution layer\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hidden_ch: (int)\n",
    "    strides: [(int, int), (int, int)]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. (default)\n",
    "          'layer_norm2d' for normalization on on FxT space \n",
    "          'batch_norm' or else, batch-normalization\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "    [Conv1x3]>>[ELU]>>[BN]>>[Conv3x1]>>[ELU]>>[BN]\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    x: (B,F,T,C) with {F=F/stride, T=T/stride, C=hidden_ch}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_ch=128,\n",
    "                 strides=[(1,1),(1,1)],\n",
    "                 norm='layer_norm2d'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv2d_1x3 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(1, 3),\n",
    "                                                 strides=strides[0],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        self.conv2d_3x1 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(3, 1),\n",
    "                                                 strides=strides[1],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        \n",
    "        if norm == 'layer_norm1d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        elif norm == 'layer_norm2d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "        else:\n",
    "            self.BN_1x3 = tf.keras.layers.BatchNormalization(axis=-1) # Fix axis: 2020 Apr20\n",
    "            self.BN_3x1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "            \n",
    "        self.forward = tf.keras.Sequential([self.conv2d_1x3,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_1x3,\n",
    "                                            self.conv2d_3x1,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_3x1\n",
    "                                            ])\n",
    "\n",
    "       \n",
    "    def call(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class DivEncLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-head projection a.k.a. 'divide and encode' layer:\n",
    "        \n",
    "    • The concept of 'divide and encode' was discovered  in Lai et.al.,\n",
    "     'Simultaneous Feature Learning and Hash Coding with Deep Neural Networks',\n",
    "      2015. https://arxiv.org/abs/1504.03410\n",
    "    • It was also adopted in Gfeller et.al. 'Now Playing: Continuo-\n",
    "      us low-power music recognition', 2017. https://arxiv.org/abs/1711.10958\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    q: (int) number of slices as 'slice_length = input_dim / q'\n",
    "    unit_dim: [(int), (int)]\n",
    "    norm: 'layer_norm1d' or 'layer_norm2d' uses 1D-layer normalization on the feature.\n",
    "          'batch_norm' or else uses batch normalization. Default is 'layer_norm2d'.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    x: (B,1,1,C)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, q=128, unit_dim=[32, 1], norm='batch_norm'):\n",
    "        super(DivEncLayer, self).__init__()\n",
    "\n",
    "        self.q = q\n",
    "        self.unit_dim = unit_dim\n",
    "        self.norm = norm\n",
    "        \n",
    "        if norm in ['layer_norm1d', 'layer_norm2d']:\n",
    "            self.BN = [tf.keras.layers.LayerNormalization(axis=-1) for i in range(q)]\n",
    "        else:\n",
    "            self.BN = [tf.keras.layers.BatchNormalization(axis=-1) for i in range(q)]\n",
    "            \n",
    "        self.split_fc_layers = self._construct_layers() \n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Prepare output embedding variable for dynamic batch-size \n",
    "        self.slice_length = int(input_shape[-1] / self.q)\n",
    "\n",
    " \n",
    "    def _construct_layers(self):\n",
    "        layers = list()\n",
    "        for i in range(self.q): # q: num_slices\n",
    "            layers.append(tf.keras.Sequential([tf.keras.layers.Dense(self.unit_dim[0], activation='elu'),\n",
    "                                               #self.BN[i],\n",
    "                                               tf.keras.layers.Dense(self.unit_dim[1])]))\n",
    "        return layers\n",
    "\n",
    " \n",
    "    @tf.function\n",
    "    def _split_encoding(self, x_slices):\n",
    "        \"\"\"\n",
    "        Input: (B,Q,S)\n",
    "        Returns: (B,Q)\n",
    "        \n",
    "        \"\"\"\n",
    "        out = list()\n",
    "        for i in range(self.q):\n",
    "            out.append(self.split_fc_layers[i](x_slices[:, i, :]))\n",
    "        return tf.concat(out, axis=1)\n",
    "\n",
    "    \n",
    "    def call(self, x): # x: (B,1,1,2048)\n",
    "        x = tf.reshape(x, shape=[x.shape[0], self.q, -1]) # (B,Q,S); Q=num_slices; S=slice length; (B,128,8 or 16)\n",
    "        return self._split_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerPrinter(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Fingerprinter: 'Neural Audio Fingerprint for High-specific Audio Retrieval\n",
    "        based on Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "    \n",
    "    IN >> [Convlayer]x8 >> [DivEncLayer] >> [L2Normalizer] >> OUT \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_shape: tuple (int), not including the batch size\n",
    "    front_hidden_ch: (list)\n",
    "    front_strides: (list)\n",
    "    emb_sz: (int) default=128\n",
    "    fc_unit_dim: (list) default=[32,1]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. \n",
    "          'layer_norm2d' for normalization on on FxT space (default).\n",
    "          'batch_norm' or else, batch-normalization.\n",
    "    use_L2layer: True (default)\n",
    "    \n",
    "    • Note: batch-normalization will not work properly with TPUs.\n",
    "                    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q) \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(256,32,1),\n",
    "                 front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024],\n",
    "                 front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)]],\n",
    "                 emb_sz=128, # q\n",
    "                 fc_unit_dim=[32,1],\n",
    "                 norm='layer_norm2d',\n",
    "                 use_L2layer=True):\n",
    "        super(FingerPrinter, self).__init__()\n",
    "        self.front_hidden_ch = front_hidden_ch\n",
    "        self.front_strides = front_strides\n",
    "        self.emb_sz=emb_sz\n",
    "        self.norm = norm\n",
    "        self.use_L2layer = use_L2layer\n",
    "        \n",
    "        self.n_clayers = len(front_strides)\n",
    "        self.front_conv = tf.keras.Sequential(name='ConvLayers')\n",
    "        if ((front_hidden_ch[-1] % emb_sz) != 0):\n",
    "            front_hidden_ch[-1] = ((front_hidden_ch[-1]//emb_sz) + 1) * emb_sz                \n",
    "        \n",
    "        # Front (sep-)conv layers\n",
    "        #x = tf.zeros((1,)+ input_shape, dtype=tf.float32)\n",
    "        #print(f\"ConvLayer entrada: {self.front_conv(x).shape}\")\n",
    "        for i in range(self.n_clayers):\n",
    "            self.front_conv.add(ConvLayer(hidden_ch=front_hidden_ch[i],\n",
    "                strides=front_strides[i], norm=norm))\n",
    "            #print(f\"ConvLayer {i+1}: {self.front_conv(x).shape}\")\n",
    "        self.front_conv.add(tf.keras.layers.Flatten()) # (B,F',T',C) >> (B,D)\n",
    "            \n",
    "        # Divide & Encoder layer\n",
    "        self.div_enc = DivEncLayer(q=emb_sz, unit_dim=fc_unit_dim, norm=norm)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n",
    "        x = self.div_enc(x) # (B,Q)\n",
    "        if self.use_L2layer:\n",
    "            return tf.math.l2_normalize(x, axis=1) \n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 32, 1) \n",
    "emb_sz = cfg['MODEL']['EMB_SZ']\n",
    "norm = cfg['MODEL']['BN']\n",
    "fc_unit_dim = [32, 1]\n",
    "\n",
    "m = FingerPrinter(input_shape=input_shape,\n",
    "                    emb_sz=emb_sz,\n",
    "                    fc_unit_dim=fc_unit_dim,\n",
    "                    norm=norm)\n",
    "m.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NTxent_loss_single_gpu import NTxentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nsteps = cfg['TRAIN']['MAX_EPOCH'] * len(train_data)\n",
    "\n",
    "lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "            initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "            decay_steps=total_nsteps,\n",
    "            alpha=1e-06)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss = NTxentLoss(\n",
    "    n_org=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "    n_rep=cfg['BSZ']['TR_BATCH_SZ'] - cfg['BSZ']['TR_N_ANCHOR'],\n",
    "    tau=cfg['LOSS']['TAU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(optimizer=opt, loss=loss, metrics=['accuracy', 'precision ', 'recall ', 'f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'slice' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m m_fp\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, train_data[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/neural-audio-fp/code-jupyter/dataloader_keras.py:225\u001b[0m, in \u001b[0;36mgenUnbalSequence.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Get anchor (original) and positive (replica) samples. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     index_anchor_for_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_event[\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_anchor\u001b[49m:(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    227\u001b[0m                                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_anchor]\n\u001b[1;32m    228\u001b[0m     Xa_batch, Xp_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__event_batch_load(index_anchor_for_batch)\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m bg_sel_indices, speech_sel_indices\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'slice' and 'int'"
     ]
    }
   ],
   "source": [
    "history = m_fp.fit(train_data, train_labels, epochs=1, batch_size = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit() to train the model, optionally providing validation data to monitor performance on unseen data.\n",
    "model.fit(train_images, train_labels, epochs=3, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def get_mnist_model(): # Create a model (we factor this into a separate function so as to reuse it later).\n",
    "  inputs = keras.Input(shape=(28 * 28,))\n",
    "  FingerPrinter(input_shape=input_shape,\n",
    "                      emb_sz=emb_sz,\n",
    "                      fc_unit_dim=fc_unit_dim,\n",
    "                      norm=norm)\n",
    "\n",
    "  model = keras.Model(inputs, outputs)\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data() # Load your data, reserving some for validation.\n",
    "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n",
    "\n",
    "\n",
    "model = get_mnist_model()\n",
    "\n",
    "# Compile the model by specifying its optimizer, the loss function to minimize, and the metrics to monitor.\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Use fit() to train the model, optionally providing validation data to monitor performance on unseen data.\n",
    "model.fit(train_images, train_labels, epochs=3, validation_data=(val_images, val_labels))\n",
    "\n",
    "\n",
    "test_metrics = model.evaluate(test_images, test_labels) # Use evaluate() to compute the loss and metrics on new data.\n",
    "predictions = model.predict(test_images) # Use predict() to compute classification probabilities on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NTxent_loss_single_gpu import NTxentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nsteps = cfg['TRAIN']['MAX_EPOCH'] * len(train_data)\n",
    "\n",
    "lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "            initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "            decay_steps=total_nsteps,\n",
    "            alpha=1e-06)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss = NTxentLoss(\n",
    "    n_org=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "    n_rep=cfg['BSZ']['TR_BATCH_SZ'] - cfg['BSZ']['TR_N_ANCHOR'],\n",
    "    tau=cfg['LOSS']['TAU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## step3: criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "ep_max = cfg['TRAIN']['MAX_EPOCH']\n",
    "for ep in range(ep_max + 1):\n",
    "    tf.print(f'EPOCH: {ep}/{ep_max}')\n",
    "\n",
    "    # Train\n",
    "    \"\"\" Parallelism to speed up preprocessing.............. \"\"\"\n",
    "    train_ds = dataset.get_train_ds(cfg['DATA_SEL']['REDUCE_ITEMS_P'])\n",
    "    progbar = Progbar(len(train_ds))\n",
    "    enq = tf.keras.utils.OrderedEnqueuer(\n",
    "        train_ds, use_multiprocessing=True, shuffle=train_ds.shuffle)\n",
    "    enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "                max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "    i = 0\n",
    "    while i < len(enq.sequence):\n",
    "        X = next(enq.get()) # X: Tuple(Xa, Xp)\n",
    "        avg_loss, sim_mtx = train_step(X, m_pre, m_specaug, m_fp,\n",
    "                                        loss_obj_train, lr_schedule)\n",
    "        progbar.add(1, values=[(\"tr loss\", avg_loss)])\n",
    "        i += 1\n",
    "    enq.stop()\n",
    "    \"\"\" End of Parallelism................................. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, m_pre, m_specaug, m_fp, loss_obj, learning_rate):\n",
    "    \"\"\" Train step \"\"\"\n",
    "    # X: (Xa, Xp)\n",
    "    # Xa: anchors or originals, s.t. [xa_0, xa_1,...]\n",
    "    # Xp: augmented replicas, s.t. [xp_0, xp_1] with xp_n = rand_aug(xa_n).\n",
    "    n_anchors = len(X[0])\n",
    "    X = tf.concat(X, axis=0)\n",
    "    feat = m_specaug(m_pre(X))  # (nA+nP, F, T, 1)\n",
    "    m_fp.trainable = True\n",
    "    with tf.GradientTape() as tape:\n",
    "        emb = m_fp(feat)  # (BSZ, Dim)\n",
    "        loss, sim_mtx, _ = loss_obj.compute_loss(\n",
    "            emb[:n_anchors, :], emb[n_anchors:, :]) # {emb_org, emb_rep}\n",
    "    #g = t.gradient(loss, m_fp.trainable_variables)\n",
    "    #helper.optimizer.apply_gradients(zip(g, m_fp.trainable_variables))\n",
    "    \n",
    "    grad_loss_wrt_m_fp = tape.gradient(loss, [m_fp]) # Retrieve the gradient of the loss with regard to weights.\n",
    "    m_fp.assign_sub(grad_loss_wrt_m_fp * learning_rate) # Update the weights.\n",
    "\n",
    "    #... # To tensorboard.\n",
    "    return loss#, sim_mtx # avg_loss: average within the current epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_specaug: spec-augmentation layer.\n",
    "    m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "    assert(m_specaug.bypass==False) # Detachable by setting m_specaug.bypass.\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    return m_pre, m_specaug, m_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre, m_specaug, m_fp = build_fp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "enq = tf.keras.utils.OrderedEnqueuer(\n",
    "            train_data, use_multiprocessing=True, shuffle=train_data.shuffle)\n",
    "\n",
    "enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "            max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 15:26:10.289903: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:933] Skipping loop optimization for Merge node with control input: sequential_137/spec_n_cutout_3/StatefulPartitionedCall/cond/branch_executed/_104\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FingerPrinter' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(enq\u001b[38;5;241m.\u001b[39msequence):\n\u001b[1;32m      3\u001b[0m   X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(enq\u001b[38;5;241m.\u001b[39mget())\n\u001b[0;32m----> 4\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_specaug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_obj_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[85], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(X, m_pre, m_specaug, m_fp, loss_obj, learning_rate)\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss, sim_mtx, _ \u001b[38;5;241m=\u001b[39m loss_obj\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[1;32m     13\u001b[0m         emb[:n_anchors, :], emb[n_anchors:, :]) \u001b[38;5;66;03m# {emb_org, emb_rep}\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#g = t.gradient(loss, m_fp.trainable_variables)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#helper.optimizer.apply_gradients(zip(g, m_fp.trainable_variables))\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m grad_loss_wrt_m_fp \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mm_fp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Retrieve the gradient of the loss with regard to weights.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m m_fp\u001b[38;5;241m.\u001b[39massign_sub(grad_loss_wrt_m_fp \u001b[38;5;241m*\u001b[39m learning_rate) \u001b[38;5;66;03m# Update the weights.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#... # To tensorboard.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FingerPrinter' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(enq.sequence):\n",
    "  X = next(enq.get())\n",
    "  loss = train_step(X, m_pre, m_specaug, m_fp, loss_obj_train, lr_schedule)\n",
    "  print(f\"Loss at step {step}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(inputs, targets):\n",
    "  with tf.GradientTape() as tape: # Forward pass, inside a gradient tape scope\n",
    "    predictions = model(inputs) # Forward pass, inside a gradient tape scope\n",
    "    loss = square_loss(predictions, targets) # Forward pass, inside a gradient tape scope\n",
    "  grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b]) # Retrieve the gradient of the loss with regard to weights.\n",
    "  W.assign_sub(grad_loss_wrt_W * learning_rate) # Update the weights.\n",
    "  b.assign_sub(grad_loss_wrt_b * learning_rate) # Update the weights.\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "        \n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "# Define a classe Linear\n",
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Define a função que cria o modelo MLP usando a Sequential API\n",
    "def create_mlp_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        Linear(32),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        Linear(32),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        Linear(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Crie o modelo MLP\n",
    "mlp_model = create_mlp_model()\n",
    "\n",
    "# Execute uma passagem de encaminhamento para construir os pesos\n",
    "y = mlp_model(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Imprima o número de pesos e pesos treináveis no modelo\n",
    "print(\"weights:\", len(mlp_model.weights))\n",
    "print(\"trainable weights:\", len(mlp_model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_960\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " linear_15 (Linear)          (3, 32)                   2080      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (3, 32)                   0         \n",
      "                                                                 \n",
      " linear_16 (Linear)          (3, 32)                   1056      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (3, 32)                   0         \n",
      "                                                                 \n",
      " linear_17 (Linear)          (3, 1)                    33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3169 (12.38 KB)\n",
      "Trainable params: 3169 (12.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
