{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train e validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import click\n",
    "import yaml\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from dataloader_keras import genUnbalSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load da config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_fname):\n",
    "    config_filepath = '../config/' + config_fname + '.yaml'\n",
    "    if os.path.exists(config_filepath):\n",
    "        print(f'cli: Configuration from {config_filepath}')\n",
    "    else:\n",
    "        sys.exit(f'cli: ERROR! Configuration file {config_filepath} is missing!!')\n",
    "\n",
    "    with open(config_filepath, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli: Configuration from ../config/default.yaml\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, mode='train'):\n",
    "        if mode == 'train': \n",
    "            _prefix = 'train-10k-30s/'\n",
    "            self.filepath = cfg['DIR']['SOURCE_ROOT_DIR'] + _prefix + '**/*.wav'\n",
    "        elif mode == 'val':\n",
    "            self.filepath='data/SHS100K-VAL'\n",
    "\n",
    "        # Data location\n",
    "        self.bg_root_dir = cfg['DIR']['BG_ROOT_DIR'] #! é este que nao preciso? ver\n",
    "        self.ir_root_dir = cfg['DIR']['IR_ROOT_DIR']\n",
    "        self.speech_root_dir = cfg['DIR']['SPEECH_ROOT_DIR']\n",
    "\n",
    "        # BSZ\n",
    "        self.tr_batch_sz = cfg['BSZ']['TR_BATCH_SZ']\n",
    "        self.tr_n_anchor = cfg['BSZ']['TR_N_ANCHOR']\n",
    "\n",
    "        # Model parameters\n",
    "        self.dur = cfg['MODEL']['DUR']\n",
    "        self.hop = cfg['MODEL']['HOP']\n",
    "        self.fs = cfg['MODEL']['FS']\n",
    "\n",
    "        # Time-domain augmentation parameter\n",
    "        self.tr_snr = cfg['TD_AUG']['TR_SNR']\n",
    "        self.ts_snr = cfg['TD_AUG']['TS_SNR']\n",
    "        self.val_snr = cfg['TD_AUG']['VAL_SNR']\n",
    "        self.tr_use_bg_aug = cfg['TD_AUG']['TR_BG_AUG']\n",
    "        self.ts_use_bg_aug = cfg['TD_AUG']['TS_BG_AUG']\n",
    "        self.val_use_bg_aug = cfg['TD_AUG']['VAL_BG_AUG']\n",
    "        self.tr_use_ir_aug = cfg['TD_AUG']['TR_IR_AUG']\n",
    "        self.ts_use_ir_aug = cfg['TD_AUG']['TS_IR_AUG']\n",
    "        self.val_use_ir_aug = cfg['TD_AUG']['VAL_IR_AUG']\n",
    "        self.tr_use_speech_aug = cfg['TD_AUG']['TR_SPEECH_AUG']\n",
    "        self.ts_use_speech_aug = cfg['TD_AUG']['TS_SPEECH_AUG']\n",
    "        self.val_use_speech_aug = cfg['TD_AUG']['VAL_SPEECH_AUG']\n",
    "\n",
    "        # Pre-load file paths for augmentation\n",
    "        self.tr_bg_fps = self.ts_bg_fps = self.val_bg_fps = None\n",
    "        self.tr_ir_fps = self.ts_ir_fps = self.val_ir_fps = None\n",
    "        self.tr_speech_fps = self.ts_speech_fps = self.val_speech_fps = None\n",
    "        self.__set_augmentation_fps()\n",
    "\n",
    "        # Source (music) file paths\n",
    "        self.tr_source_fps = self.val_source_fps = None\n",
    "\n",
    "\n",
    "    def train_ds(self):    \n",
    "        self.tr_source_fps = sorted(glob.glob(self.filepath, recursive=True))\n",
    "        \n",
    "        reduce_items_p = cfg['DATA_SEL']['REDUCE_ITEMS_P']\n",
    "\n",
    "        ds = genUnbalSequence(\n",
    "            fns_event_list=self.tr_source_fps,\n",
    "            bsz=self.tr_batch_sz,\n",
    "            n_anchor=self.tr_n_anchor, #ex) bsz=40, n_anchor=8: 4 positive samples per anchor\n",
    "            duration=self.dur,  # duration in seconds\n",
    "            hop=self.hop,\n",
    "            fs=self.fs,\n",
    "            shuffle=True,\n",
    "            random_offset_anchor=True,\n",
    "            bg_mix_parameter=[self.tr_use_bg_aug, self.tr_bg_fps, self.tr_snr],\n",
    "            ir_mix_parameter=[self.tr_use_ir_aug, self.tr_ir_fps],\n",
    "            speech_mix_parameter=[self.tr_use_speech_aug, self.tr_speech_fps,\n",
    "                                  self.tr_snr],\n",
    "            reduce_items_p=reduce_items_p)\n",
    "        return ds\n",
    "    \n",
    "    def __set_augmentation_fps(self):\n",
    "        \"\"\"\n",
    "        Set file path lists:\n",
    "\n",
    "            If validation set was not available, we replace it with subset of\n",
    "            the trainset.\n",
    "\n",
    "        \"\"\"\n",
    "        # File lists for Augmentations\n",
    "        if self.tr_use_bg_aug:\n",
    "            self.tr_bg_fps = sorted(glob.glob(self.bg_root_dir +\n",
    "                                              'tr/**/*.wav', recursive=True))\n",
    "        if self.ts_use_bg_aug:\n",
    "            self.ts_bg_fps = sorted(glob.glob(self.bg_root_dir +\n",
    "                                              'ts/**/*.wav', recursive=True))\n",
    "        if self.val_use_bg_aug:\n",
    "            self.val_bg_fps = sorted(glob.glob(self.bg_root_dir +\n",
    "                                               'tr/**/*.wav', recursive=True))\n",
    "\n",
    "        if self.tr_use_ir_aug:\n",
    "            self.tr_ir_fps = sorted(\n",
    "                glob.glob(self.ir_root_dir + 'tr/**/*.wav', recursive=True))\n",
    "        if self.ts_use_ir_aug:\n",
    "            self.ts_ir_fps = sorted(\n",
    "                glob.glob(self.ir_root_dir + 'ts/**/*.wav', recursive=True))\n",
    "        if self.val_use_ir_aug:\n",
    "            self.val_ir_fps = sorted(\n",
    "                glob.glob(self.ir_root_dir + 'tr/**/*.wav', recursive=True))\n",
    "\n",
    "        if self.tr_use_speech_aug:\n",
    "            self.tr_speech_fps = sorted(\n",
    "                glob.glob(self.speech_root_dir + 'train/**/*.wav',\n",
    "                          recursive=True))\n",
    "        self.ts_speech_fps = sorted(\n",
    "            glob.glob(self.speech_root_dir + 'test/**/*.wav',\n",
    "                      recursive=True))\n",
    "        if self.val_use_speech_aug:\n",
    "            self.val_speech_fps = sorted(\n",
    "                glob.glob(self.speech_root_dir + 'dev/**/*.wav',\n",
    "                          recursive=True))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset('train')\n",
    "train_data = data.train_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próximo passo, meter o dados mais visíveis. Para caso eu queira ver uma musica, seja só fazer um print no main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melspectrogram import get_melspec_layer\n",
    "from specaug_chain import get_specaug_chain_layer\n",
    "from nnfp import get_fingerprinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "# m_specaug: spec-augmentation layer.\n",
    "m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "\n",
    "m_fp = get_fingerprinter(cfg, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Separable convolution layer\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hidden_ch: (int)\n",
    "    strides: [(int, int), (int, int)]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. (default)\n",
    "          'layer_norm2d' for normalization on on FxT space \n",
    "          'batch_norm' or else, batch-normalization\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "    [Conv1x3]>>[ELU]>>[BN]>>[Conv3x1]>>[ELU]>>[BN]\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    x: (B,F,T,C) with {F=F/stride, T=T/stride, C=hidden_ch}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_ch=128,\n",
    "                 strides=[(1,1),(1,1)],\n",
    "                 norm='layer_norm2d'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv2d_1x3 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(1, 3),\n",
    "                                                 strides=strides[0],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        self.conv2d_3x1 = tf.keras.layers.Conv2D(hidden_ch,\n",
    "                                                 kernel_size=(3, 1),\n",
    "                                                 strides=strides[1],\n",
    "                                                 padding='SAME',\n",
    "                                                 dilation_rate=(1, 1),\n",
    "                                                 kernel_initializer='glorot_uniform',\n",
    "                                                 bias_initializer='zeros')\n",
    "        \n",
    "        if norm == 'layer_norm1d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        elif norm == 'layer_norm2d':\n",
    "            self.BN_1x3 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "            self.BN_3x1 = tf.keras.layers.LayerNormalization(axis=(1, 2, 3))\n",
    "        else:\n",
    "            self.BN_1x3 = tf.keras.layers.BatchNormalization(axis=-1) # Fix axis: 2020 Apr20\n",
    "            self.BN_3x1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "            \n",
    "        self.forward = tf.keras.Sequential([self.conv2d_1x3,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_1x3,\n",
    "                                            self.conv2d_3x1,\n",
    "                                            tf.keras.layers.ELU(),\n",
    "                                            self.BN_3x1\n",
    "                                            ])\n",
    "\n",
    "       \n",
    "    def call(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivEncLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-head projection a.k.a. 'divide and encode' layer:\n",
    "        \n",
    "    • The concept of 'divide and encode' was discovered  in Lai et.al.,\n",
    "     'Simultaneous Feature Learning and Hash Coding with Deep Neural Networks',\n",
    "      2015. https://arxiv.org/abs/1504.03410\n",
    "    • It was also adopted in Gfeller et.al. 'Now Playing: Continuo-\n",
    "      us low-power music recognition', 2017. https://arxiv.org/abs/1711.10958\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    q: (int) number of slices as 'slice_length = input_dim / q'\n",
    "    unit_dim: [(int), (int)]\n",
    "    norm: 'layer_norm1d' or 'layer_norm2d' uses 1D-layer normalization on the feature.\n",
    "          'batch_norm' or else uses batch normalization. Default is 'layer_norm2d'.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    x: (B,1,1,C)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, q=128, unit_dim=[32, 1], norm='batch_norm'):\n",
    "        super(DivEncLayer, self).__init__()\n",
    "\n",
    "        self.q = q\n",
    "        self.unit_dim = unit_dim\n",
    "        self.norm = norm\n",
    "        \n",
    "        if norm in ['layer_norm1d', 'layer_norm2d']:\n",
    "            self.BN = [tf.keras.layers.LayerNormalization(axis=-1) for i in range(q)]\n",
    "        else:\n",
    "            self.BN = [tf.keras.layers.BatchNormalization(axis=-1) for i in range(q)]\n",
    "            \n",
    "        \n",
    "        self.split_fc_layers = self._construct_layers() \n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Prepare output embedding variable for dynamic batch-size \n",
    "        self.slice_length = int(input_shape[-1] / self.q)\n",
    "\n",
    " \n",
    "    def _construct_layers(self):\n",
    "        layers = list()\n",
    "        for i in range(self.q): # q: num_slices\n",
    "            layers.append(tf.keras.Sequential([tf.keras.layers.Dense(self.unit_dim[0], activation='elu'),\n",
    "                                               #self.BN[i],\n",
    "                                               tf.keras.layers.Dense(self.unit_dim[1])]))\n",
    "        return layers\n",
    "\n",
    " \n",
    "    @tf.function\n",
    "    def _split_encoding(self, x_slices):\n",
    "        \"\"\"\n",
    "        Input: (B,Q,S)\n",
    "        Returns: (B,Q)\n",
    "        \n",
    "        \"\"\"\n",
    "        out = list()\n",
    "        for i in range(self.q):\n",
    "            out.append(self.split_fc_layers[i](x_slices[:, i, :]))\n",
    "        return tf.concat(out, axis=1)\n",
    "\n",
    "    \n",
    "    def call(self, x): # x: (B,1,1,2048)\n",
    "        x = tf.reshape(x, shape=[x.shape[0], self.q, -1]) # (B,Q,S); Q=num_slices; S=slice length; (B,128,8 or 16)\n",
    "        return self._split_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerPrinter(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Fingerprinter: 'Neural Audio Fingerprint for High-specific Audio Retrieval\n",
    "        based on Contrastive Learning', https://arxiv.org/abs/2010.11910\n",
    "    \n",
    "    IN >> [Convlayer]x8 >> [DivEncLayer] >> [L2Normalizer] >> OUT \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_shape: tuple (int), not including the batch size\n",
    "    front_hidden_ch: (list)\n",
    "    front_strides: (list)\n",
    "    emb_sz: (int) default=128\n",
    "    fc_unit_dim: (list) default=[32,1]\n",
    "    norm: 'layer_norm1d' for normalization on Freq axis. \n",
    "          'layer_norm2d' for normalization on on FxT space (default).\n",
    "          'batch_norm' or else, batch-normalization.\n",
    "    use_L2layer: True (default)\n",
    "    \n",
    "    • Note: batch-normalization will not work properly with TPUs.\n",
    "                    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    x: (B,F,T,1)\n",
    "    \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    emb: (B,Q) \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(256,32,1),\n",
    "                 front_hidden_ch=[128, 128, 256, 256, 512, 512, 1024, 1024],\n",
    "                 front_strides=[[(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,2), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)],\n",
    "                                [(1,1), (2,1)], [(1,2), (2,1)]],\n",
    "                 emb_sz=128, # q\n",
    "                 fc_unit_dim=[32,1],\n",
    "                 norm='layer_norm2d',\n",
    "                 use_L2layer=True):\n",
    "        super(FingerPrinter, self).__init__()\n",
    "        self.front_hidden_ch = front_hidden_ch\n",
    "        self.front_strides = front_strides\n",
    "        self.emb_sz=emb_sz\n",
    "        self.norm = norm\n",
    "        self.use_L2layer = use_L2layer\n",
    "        \n",
    "        self.n_clayers = len(front_strides)\n",
    "        self.front_conv = tf.keras.Sequential(name='ConvLayers')\n",
    "        if ((front_hidden_ch[-1] % emb_sz) != 0):\n",
    "            front_hidden_ch[-1] = ((front_hidden_ch[-1]//emb_sz) + 1) * emb_sz                \n",
    "        \n",
    "        # Front (sep-)conv layers\n",
    "        for i in range(self.n_clayers):\n",
    "            self.front_conv.add(ConvLayer(hidden_ch=front_hidden_ch[i],\n",
    "                strides=front_strides[i], norm=norm))\n",
    "        self.front_conv.add(tf.keras.layers.Flatten()) # (B,F',T',C) >> (B,D)\n",
    "            \n",
    "        # Divide & Encoder layer\n",
    "        self.div_enc = DivEncLayer(q=emb_sz, unit_dim=fc_unit_dim, norm=norm)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n",
    "        x = self.div_enc(x) # (B,Q)\n",
    "        if self.use_L2layer:\n",
    "            return tf.math.l2_normalize(x, axis=1) \n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprinter(cfg, trainable=False):\n",
    "    \"\"\"\n",
    "    Input length : 1s or 2s\n",
    "    \n",
    "    Arguements\n",
    "    ----------\n",
    "    cfg : (dict)\n",
    "        created from the '.yaml' located in /config dicrectory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    <tf.keras.Model> FingerPrinter object\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape = (256, 32, 1) \n",
    "    emb_sz = cfg['MODEL']['EMB_SZ']\n",
    "    norm = cfg['MODEL']['BN']\n",
    "    fc_unit_dim = [32, 1]\n",
    "    \n",
    "    m = FingerPrinter(input_shape=input_shape,\n",
    "                      emb_sz=emb_sz,\n",
    "                      fc_unit_dim=fc_unit_dim,\n",
    "                      norm=norm)\n",
    "    m.trainable = trainable\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NTxent_loss_single_gpu import NTxentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nsteps = cfg['TRAIN']['MAX_EPOCH'] * len(train_data)\n",
    "\n",
    "lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "            initial_learning_rate=float(cfg['TRAIN']['LR']),\n",
    "            decay_steps=total_nsteps,\n",
    "            alpha=1e-06)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss = NTxentLoss(\n",
    "    n_org=cfg['BSZ']['TR_N_ANCHOR'],\n",
    "    n_rep=cfg['BSZ']['TR_BATCH_SZ'] - cfg['BSZ']['TR_N_ANCHOR'],\n",
    "    tau=cfg['LOSS']['TAU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fp.compile(optimizer=opt, loss=loss, metrics=['accuracy', 'precision ', 'recall ', 'f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file5rpfk4re.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).front_conv, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_file9xib_vf8.py\", line 15, in tf__call\n        raise\n\n    ValueError: Exception encountered when calling layer 'finger_printer_4' (type FingerPrinter).\n    \n    in user code:\n    \n        File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 229, in call  *\n            x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n        File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_file9xib_vf8.py\", line 15, in tf__call\n            raise\n    \n        ValueError: Exception encountered when calling layer 'conv_layer_32' (type ConvLayer).\n        \n        in user code:\n        \n            File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 83, in call  *\n                return self.forward(x)\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n                raise ValueError(\n        \n            ValueError: Exception encountered when calling layer 'sequential_548' (type Sequential).\n            \n            Input 0 of layer \"conv2d_64\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, None, None)\n            \n            Call arguments received by layer 'sequential_548' (type Sequential):\n              • inputs=tf.Tensor(shape=(None, None, None), dtype=float32)\n              • training=True\n              • mask=None\n        \n        \n        Call arguments received by layer 'conv_layer_32' (type ConvLayer):\n          • x=tf.Tensor(shape=(None, None, None), dtype=float32)\n    \n    \n    Call arguments received by layer 'finger_printer_4' (type FingerPrinter):\n      • inputs=tf.Tensor(shape=(None, None, None), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mm_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filey0kz7q0c.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5rpfk4re.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfront_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv_enc, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file9xib_vf8.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mforward, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file5rpfk4re.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).front_conv, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_file9xib_vf8.py\", line 15, in tf__call\n        raise\n\n    ValueError: Exception encountered when calling layer 'finger_printer_4' (type FingerPrinter).\n    \n    in user code:\n    \n        File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 229, in call  *\n            x = self.front_conv(inputs) # (B,D) with D = (T/2^4) x last_hidden_ch\n        File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_file9xib_vf8.py\", line 15, in tf__call\n            raise\n    \n        ValueError: Exception encountered when calling layer 'conv_layer_32' (type ConvLayer).\n        \n        in user code:\n        \n            File \"/home/rodrigo/Documents/neural-audio-fp/model/fp/nnfp.py\", line 83, in call  *\n                return self.forward(x)\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/home/rodrigo/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n                raise ValueError(\n        \n            ValueError: Exception encountered when calling layer 'sequential_548' (type Sequential).\n            \n            Input 0 of layer \"conv2d_64\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, None, None)\n            \n            Call arguments received by layer 'sequential_548' (type Sequential):\n              • inputs=tf.Tensor(shape=(None, None, None), dtype=float32)\n              • training=True\n              • mask=None\n        \n        \n        Call arguments received by layer 'conv_layer_32' (type ConvLayer):\n          • x=tf.Tensor(shape=(None, None, None), dtype=float32)\n    \n    \n    Call arguments received by layer 'finger_printer_4' (type FingerPrinter):\n      • inputs=tf.Tensor(shape=(None, None, None), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "history = m_fp.fit(train_data, epochs=1, batch_size = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## step3: criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "ep_max = cfg['TRAIN']['MAX_EPOCH']\n",
    "for ep in range(ep_max + 1):\n",
    "    tf.print(f'EPOCH: {ep}/{ep_max}')\n",
    "\n",
    "    # Train\n",
    "    \"\"\" Parallelism to speed up preprocessing.............. \"\"\"\n",
    "    train_ds = dataset.get_train_ds(cfg['DATA_SEL']['REDUCE_ITEMS_P'])\n",
    "    progbar = Progbar(len(train_ds))\n",
    "    enq = tf.keras.utils.OrderedEnqueuer(\n",
    "        train_ds, use_multiprocessing=True, shuffle=train_ds.shuffle)\n",
    "    enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "                max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])\n",
    "    i = 0\n",
    "    while i < len(enq.sequence):\n",
    "        X = next(enq.get()) # X: Tuple(Xa, Xp)\n",
    "        avg_loss, sim_mtx = train_step(X, m_pre, m_specaug, m_fp,\n",
    "                                        loss_obj_train, lr_schedule)\n",
    "        progbar.add(1, values=[(\"tr loss\", avg_loss)])\n",
    "        i += 1\n",
    "    enq.stop()\n",
    "    \"\"\" End of Parallelism................................. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, m_pre, m_specaug, m_fp, loss_obj, learning_rate):\n",
    "    \"\"\" Train step \"\"\"\n",
    "    # X: (Xa, Xp)\n",
    "    # Xa: anchors or originals, s.t. [xa_0, xa_1,...]\n",
    "    # Xp: augmented replicas, s.t. [xp_0, xp_1] with xp_n = rand_aug(xa_n).\n",
    "    n_anchors = len(X[0])\n",
    "    X = tf.concat(X, axis=0)\n",
    "    feat = m_specaug(m_pre(X))  # (nA+nP, F, T, 1)\n",
    "    m_fp.trainable = True\n",
    "    with tf.GradientTape() as tape:\n",
    "        emb = m_fp(feat)  # (BSZ, Dim)\n",
    "        loss, sim_mtx, _ = loss_obj.compute_loss(\n",
    "            emb[:n_anchors, :], emb[n_anchors:, :]) # {emb_org, emb_rep}\n",
    "    #g = t.gradient(loss, m_fp.trainable_variables)\n",
    "    #helper.optimizer.apply_gradients(zip(g, m_fp.trainable_variables))\n",
    "    \n",
    "    grad_loss_wrt_m_fp = tape.gradient(loss, [m_fp]) # Retrieve the gradient of the loss with regard to weights.\n",
    "    m_fp.assign_sub(grad_loss_wrt_m_fp * learning_rate) # Update the weights.\n",
    "\n",
    "    #... # To tensorboard.\n",
    "    return loss#, sim_mtx # avg_loss: average within the current epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fp(cfg):\n",
    "    \"\"\" Build fingerprinter \"\"\"\n",
    "    # m_pre: log-power-Mel-spectrogram layer, S.\n",
    "    m_pre = get_melspec_layer(cfg, trainable=False)\n",
    "\n",
    "    # m_specaug: spec-augmentation layer.\n",
    "    m_specaug = get_specaug_chain_layer(cfg, trainable=False)\n",
    "    assert(m_specaug.bypass==False) # Detachable by setting m_specaug.bypass.\n",
    "\n",
    "    # m_fp: fingerprinter g(f(.)).\n",
    "    m_fp = get_fingerprinter(cfg, trainable=False)\n",
    "    return m_pre, m_specaug, m_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre, m_specaug, m_fp = build_fp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "enq = tf.keras.utils.OrderedEnqueuer(\n",
    "            train_data, use_multiprocessing=True, shuffle=train_data.shuffle)\n",
    "\n",
    "enq.start(workers=cfg['DEVICE']['CPU_N_WORKERS'],\n",
    "            max_queue_size=cfg['DEVICE']['CPU_MAX_QUEUE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 15:26:10.289903: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:933] Skipping loop optimization for Merge node with control input: sequential_137/spec_n_cutout_3/StatefulPartitionedCall/cond/branch_executed/_104\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FingerPrinter' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(enq\u001b[38;5;241m.\u001b[39msequence):\n\u001b[1;32m      3\u001b[0m   X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(enq\u001b[38;5;241m.\u001b[39mget())\n\u001b[0;32m----> 4\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_specaug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_obj_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[85], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(X, m_pre, m_specaug, m_fp, loss_obj, learning_rate)\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss, sim_mtx, _ \u001b[38;5;241m=\u001b[39m loss_obj\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[1;32m     13\u001b[0m         emb[:n_anchors, :], emb[n_anchors:, :]) \u001b[38;5;66;03m# {emb_org, emb_rep}\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#g = t.gradient(loss, m_fp.trainable_variables)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#helper.optimizer.apply_gradients(zip(g, m_fp.trainable_variables))\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m grad_loss_wrt_m_fp \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mm_fp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Retrieve the gradient of the loss with regard to weights.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m m_fp\u001b[38;5;241m.\u001b[39massign_sub(grad_loss_wrt_m_fp \u001b[38;5;241m*\u001b[39m learning_rate) \u001b[38;5;66;03m# Update the weights.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#... # To tensorboard.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FingerPrinter' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(enq.sequence):\n",
    "  X = next(enq.get())\n",
    "  loss = train_step(X, m_pre, m_specaug, m_fp, loss_obj_train, lr_schedule)\n",
    "  print(f\"Loss at step {step}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(inputs, targets):\n",
    "  with tf.GradientTape() as tape: # Forward pass, inside a gradient tape scope\n",
    "    predictions = model(inputs) # Forward pass, inside a gradient tape scope\n",
    "    loss = square_loss(predictions, targets) # Forward pass, inside a gradient tape scope\n",
    "  grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b]) # Retrieve the gradient of the loss with regard to weights.\n",
    "  W.assign_sub(grad_loss_wrt_W * learning_rate) # Update the weights.\n",
    "  b.assign_sub(grad_loss_wrt_b * learning_rate) # Update the weights.\n",
    "  return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
